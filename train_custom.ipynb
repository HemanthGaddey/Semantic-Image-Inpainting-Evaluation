{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.data_loader import dataloader\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self, name, model, mask_type, img_file, mask_file, text_config, batchSize, prior_alpha, prior_beta, lr_policy, lr, gan_mode):\n",
    "        self.name=name\n",
    "        self.model=model\n",
    "        self.mask_type=mask_type\n",
    "        self.img_file=img_file # has paths of all the images\n",
    "        self.mask_file=mask_file\n",
    "        self.text_config=text_config\n",
    "        self.batchSize=batchSize\n",
    "        self.prior_alpha=prior_alpha\n",
    "        self.prior_bets=prior_beta\n",
    "        self.lr_policy=lr_policy\n",
    "        self.lr=lr\n",
    "        self.gan_mode=gan_mode\n",
    "\n",
    "opt=Options(\"tdanet\", 0, './datasets/CUB_200_2011/train.flist', './datasets/CUB_200_2011/train_mask.flist', 'config.bird.yml', 10, 0.8, 8, 'lambda', 1e-4,\n",
    "            'lsgan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = {\n",
    "    'MAX_TEXT_LENGTH' : 128,\n",
    "\n",
    "    'VOCAB' : \"./datasets/captions_vocab_bird.pickle\",      # The path to DAMSM vocab pickle file\n",
    "    'LANGUAGE_ENCODER' : \"./datasets/text_encoder_bird.pth\",    # The path to DAMSM text encoder\n",
    "    'IMAGE_ENCODER': \"./datasets/image_encoder_bird.pth\",   # The path to DAMSM image encoder\n",
    "    'EMBEDDING_DIM' : 256,\n",
    "\n",
    "    'CATE_IMAGE_TRAIN' : \"./datasets/CUB_200_2011/cate_image_train.json\",   # The path to category-image mapping cache file\n",
    "    'IMAGE_CATE_TRAIN' : \"./datasets/CUB_200_2011/image_cate.json\",     # The path to image-category mapping cache file\n",
    "\n",
    "    'CAPTION' : \"./datasets/CUB_200_2011/caption.json\",     # The path to image-caption cache file\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generating masks\n",
    "def scale_pyramid(img, num_scales):\n",
    "    scaled_imgs = [img]\n",
    "\n",
    "    s = img.size()\n",
    "\n",
    "    h = s[2]\n",
    "    w = s[3]\n",
    "\n",
    "    for i in range(1, num_scales):\n",
    "        ratio = 2**i\n",
    "        nh = h // ratio\n",
    "        nw = w // ratio\n",
    "        scaled_img = scale_img(img, size=[nh, nw])\n",
    "        scaled_imgs.append(scaled_img)\n",
    "\n",
    "    scaled_imgs.reverse()\n",
    "    return scaled_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= dataloader(opt)\n",
    "dataset_size = len(dataset) * opt.batchSize\n",
    "\n",
    "\n",
    "class TDAnet:\n",
    "    def __init__(self, opt):\n",
    "        \n",
    "        self.loss_names = ['kl_rec', 'kl_g', 'l1_rec', 'l1_g', 'gan_g', 'word_g', 'sentence_g', 'ad_l2_g',\n",
    "                           'gan_rec', 'ad_l2_rec', 'word_rec', 'sentence_rec',  'dis_img', 'dis_img_rec']\n",
    "        self.log_names = []\n",
    "        self.visual_names = ['img_m', 'img_truth', 'img_c', 'img_out', 'img_g', 'img_rec']\n",
    "        self.text_names = ['text_positive']\n",
    "        self.value_names = ['u_m', 'sigma_m', 'u_post', 'sigma_post', 'u_prior', 'sigma_prior']\n",
    "        self.model_names = ['E', 'G', 'D', 'D_rec']\n",
    "        self.distribution = []\n",
    "        self.prior_alpha = opt.prior_alpha\n",
    "        self.prior_beta = opt.prior_beta\n",
    "        self.max_pool = None if opt.no_maxpooling else 'max'\n",
    "\n",
    "        # inpainting model\n",
    "        self.net_E = network.define_att_textual_e(ngf=32, z_nc=256, img_f=256, layers=5, norm='none', activation='LeakyReLU',\n",
    "                          init_type='orthogonal', gpu_ids=opt.gpu_ids, image_dim=256, text_dim=256, multi_peak=False, pool_attention=self.max_pool)\n",
    "        self.net_G = network.define_hidden_textual_g(f_text_dim=768, ngf=32, z_nc=256, img_f=256, L=0, layers=5, output_scale=opt.output_scale,\n",
    "                                      norm='instance', activation='LeakyReLU', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
    "        \n",
    "        #discriminator model\n",
    "        self.net_D = network.define_d(ndf=32, img_f=128, layers=5, model_type='ResDis', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
    "        self.net_D_rec = network.define_d(ndf=32, img_f=128, layers=5, model_type='ResDis', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
    "\n",
    "        self._init_language_model(DEFAULT_CONFIG)\n",
    "\n",
    "        if self.isTrain:\n",
    "            # define the loss functions\n",
    "            self.GANloss = external_function.GANLoss(opt.gan_mode)\n",
    "            self.L1loss = torch.nn.L1Loss()\n",
    "            self.L2loss = torch.nn.MSELoss()\n",
    "\n",
    "            self.image_encoder = network.CNN_ENCODER(DEFAULT_CONFIG['EMBEDDING_DIM'])\n",
    "            state_dict = torch.load(\n",
    "                DEFAULT_CONFIG['IMAGE_ENCODER'], map_location=lambda storage, loc: storage)\n",
    "            self.image_encoder.load_state_dict(state_dict)\n",
    "            self.image_encoder.eval()\n",
    "            if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "                self.image_encoder.cuda()\n",
    "            base_function._freeze(self.image_encoder)\n",
    "\n",
    "            # define the optimizer\n",
    "            self.optimizer_G = torch.optim.Adam(itertools.chain(filter(lambda p: p.requires_grad, self.net_G.parameters()),\n",
    "                        filter(lambda p: p.requires_grad, self.net_E.parameters())), lr=opt.lr, betas=(0.0, 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(itertools.chain(filter(lambda p: p.requires_grad, self.net_D.parameters()),\n",
    "                                                filter(lambda p: p.requires_grad, self.net_D_rec.parameters())),\n",
    "                                                lr=opt.lr, betas=(0.0, 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "        self.setup(opt)\n",
    "\n",
    "        def _init_language_model(self, text_config):\n",
    "            x = pickle.load(open(text_config['VOCAB'], 'rb'))\n",
    "            self.ixtoword = x[2]\n",
    "            self.wordtoix = x[3]\n",
    "\n",
    "            word_len = len(self.wordtoix)\n",
    "            self.text_encoder = network.RNN_ENCODER(word_len, nhidden=256)\n",
    "\n",
    "            state_dict = torch.load(text_config.LANGUAGE_ENCODER, map_location=lambda storage, loc: storage)\n",
    "            self.text_encoder.load_state_dict(state_dict)\n",
    "            self.text_encoder.eval()\n",
    "            if not self.opt.update_language:\n",
    "                self.text_encoder.requires_grad_(False)\n",
    "            if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "                self.text_encoder.cuda()\n",
    "\n",
    "        def set_input(self, input, epoch=0):\n",
    "            \"\"\"Get set of data from DataLoader and do necessary preprocessing\"\"\"\n",
    "            self.input = input\n",
    "            self.image_paths = self.input['img_path']\n",
    "            self.img = input['img']\n",
    "            self.mask = input['mask']\n",
    "            self.caption_idx = input['caption_idx']\n",
    "            self.caption_length = input['caption_len']\n",
    "\n",
    "            if len(self.gpu_ids) > 0:\n",
    "                self.img = self.img.cuda(self.gpu_ids[0], True)\n",
    "                self.mask = self.mask.cuda(self.gpu_ids[0], True)\n",
    "\n",
    "            # get I_m and I_c for image with mask and complement regions for training\n",
    "            self.img_truth = self.img * 2 - 1\n",
    "            self.img_m = self.mask * self.img_truth\n",
    "            self.img_c =  (1 - self.mask) * self.img_truth\n",
    "\n",
    "            # get multiple scales image ground truth and mask for training\n",
    "            self.scale_img = scale_pyramid(self.img_truth, self.opt.output_scale)\n",
    "            self.scale_mask = scale_pyramid(self.mask, self.opt.output_scale)\n",
    "\n",
    "            # About text stuff\n",
    "            self.text_positive = util.idx_to_caption(\n",
    "                                        self.ixtoword, self.caption_idx[0].tolist(), self.caption_length[0].item())\n",
    "            self.word_embeddings, self.sentence_embedding = util.vectorize_captions_idx_batch(\n",
    "                                                        self.caption_idx, self.caption_length, self.text_encoder)\n",
    "            self.text_mask = util.lengths_to_mask(self.caption_length, max_length=self.word_embeddings.size(-1))\n",
    "            self.match_labels = torch.LongTensor(range(len(self.img_m)))\n",
    "            if len(self.gpu_ids) > 0:\n",
    "                self.word_embeddings = self.word_embeddings.cuda(self.gpu_ids[0], True)\n",
    "                self.sentence_embedding = self.sentence_embedding.cuda(self.gpu_ids[0], True)\n",
    "                self.text_mask = self.text_mask.cuda(self.gpu_ids[0], True)\n",
    "                self.match_labels = self.match_labels.cuda(self.gpu_ids[0], True)\n",
    "                \n",
    "        def forward(self):\n",
    "            \"\"\"Perform forward propagation of given inputs\"\"\"\n",
    "            # encoder process\n",
    "            distribution_factors, f, f_text = self.net_E(self.img_m, self.sentence_embedding, self.word_embeddings, self.text_mask, self.mask, self.img_c)\n",
    "            p_distribution, q_distribution, self.kl_rec, self.kl_g = self.get_distribution(distribution_factors)\n",
    "\n",
    "            # decoder process\n",
    "            z, f_m, f_e, mask = self.get_G_inputs(p_distribution, q_distribution, f) # prepare inputs: img, mask, distribute\n",
    "\n",
    "            results, attn = self.net_G(z, f_text, f_e, mask)\n",
    "            self.img_rec = []\n",
    "            self.img_g = []\n",
    "            for result in results:\n",
    "                img_rec, img_g = result.chunk(2)\n",
    "                self.img_rec.append(img_rec)\n",
    "                self.img_g.append(img_g)\n",
    "            self.img_out = (1-self.mask) * self.img_g[-1].detach() + self.mask * self.img_truth\n",
    "\n",
    "            self.region_features_rec, self.cnn_code_rec = self.image_encoder(self.img_rec[-1])\n",
    "            self.region_features_g, self.cnn_code_g = self.image_encoder(self.img_g[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={\"a\":1,\"b\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TDAnet(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
