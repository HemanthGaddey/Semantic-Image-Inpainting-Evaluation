{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4af9ed5-e9a3-4844-9f4f-44e3587945d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from external_function import SpectralNorm\n",
    "import os, ntpath\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from util import util\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from util import task\n",
    "from util import task, util\n",
    "import itertools\n",
    "from options.global_config import TextConfig\n",
    "import pickle\n",
    "import importlib\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from image_folder import make_dataset\n",
    "from util import task, util\n",
    "from global_config import TextConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f13e9c-0ea7-4a1c-a008-724e3110e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', gain=0.02):\n",
    "    \"\"\"Get different initial method for the network weights\"\"\"\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv')!=-1 or classname.find('Linear')!=-1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n",
    "\n",
    "def get_norm_layer(norm_type='batch'):\n",
    "    \"\"\"Get the normalization layer for the networks\"\"\"\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, momentum=0.1, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=True)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = None\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "\n",
    "def get_nonlinearity_layer(activation_type='PReLU'):\n",
    "    \"\"\"Get the activation layer for the networks\"\"\"\n",
    "    if activation_type == 'ReLU':\n",
    "        nonlinearity_layer = nn.ReLU()\n",
    "    elif activation_type == 'SELU':\n",
    "        nonlinearity_layer = nn.SELU()\n",
    "    elif activation_type == 'LeakyReLU':\n",
    "        nonlinearity_layer = nn.LeakyReLU(0.1)\n",
    "    elif activation_type == 'PReLU':\n",
    "        nonlinearity_layer = nn.PReLU()\n",
    "    else:\n",
    "        raise NotImplementedError('activation layer [%s] is not found' % activation_type)\n",
    "    return nonlinearity_layer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "    \"\"\"Get the training learning rate for different epoch\"\"\"\n",
    "    if opt.lr_policy == 'lambda':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch+1+1+opt.iter_count-opt.niter) / float(opt.niter_decay+1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt.lr_policy == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
    "    elif opt.lr_policy == 'exponent':\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    else:\n",
    "        raise NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def print_network(net):\n",
    "    \"\"\"print the network\"\"\"\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('total number of parameters: %.3f M' % (num_params/1e6))\n",
    "\n",
    "\n",
    "def init_net(net, init_type='normal', activation='relu', gpu_ids=[]):\n",
    "    \"\"\"print the network structure and initial the network\"\"\"\n",
    "    print_network(net)\n",
    "\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert(torch.cuda.is_available())\n",
    "        net.cuda()\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)\n",
    "    init_weights(net, init_type)\n",
    "    return net\n",
    "\n",
    "\n",
    "def _freeze(*args):\n",
    "    \"\"\"freeze the network for forward process\"\"\"\n",
    "    for module in args:\n",
    "        if module:\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "\n",
    "def _unfreeze(*args):\n",
    "    \"\"\" unfreeze the network for parameter update\"\"\"\n",
    "    for module in args:\n",
    "        if module:\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "\n",
    "def spectral_norm(module, use_spect=True):\n",
    "    \"\"\"use spectral normal layer to stable the training process\"\"\"\n",
    "    if use_spect:\n",
    "        return SpectralNorm(module)\n",
    "    else:\n",
    "        return module\n",
    "\n",
    "\n",
    "def coord_conv(input_nc, output_nc, use_spect=False, use_coord=False, with_r=False, **kwargs):\n",
    "    \"\"\"use coord convolution layer to add position information\"\"\"\n",
    "    if use_coord:\n",
    "        return CoordConv(input_nc, output_nc, with_r, use_spect, **kwargs)\n",
    "    else:\n",
    "        return spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)\n",
    "\n",
    "def conv1x1(in_planes, out_planes):\n",
    "    \"1x1 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                     padding=0, bias=False)\n",
    "\n",
    "def func_attention(query, context, gamma1):\n",
    "    \"\"\"\n",
    "    query: batch x ndf x queryL\n",
    "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
    "    mask: batch_size x sourceL\n",
    "    \"\"\"\n",
    "    batch_size, queryL = query.size(0), query.size(2)\n",
    "    ih, iw = context.size(2), context.size(3)\n",
    "    sourceL = ih * iw\n",
    "\n",
    "    # --> batch x sourceL x ndf\n",
    "    context = context.view(batch_size, -1, sourceL)\n",
    "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
    "\n",
    "    # Get attention\n",
    "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
    "    # -->batch x sourceL x queryL\n",
    "    attn = torch.bmm(contextT, query)  # Eq. (7) in AttnGAN paper\n",
    "    # --> batch*sourceL x queryL\n",
    "    attn = attn.view(batch_size * sourceL, queryL)\n",
    "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
    "\n",
    "    # --> batch x sourceL x queryL\n",
    "    attn = attn.view(batch_size, sourceL, queryL)\n",
    "    # --> batch*queryL x sourceL\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "    attn = attn.view(batch_size * queryL, sourceL)\n",
    "    #  Eq. (9)\n",
    "    attn = attn * gamma1\n",
    "    attn = nn.Softmax()(attn)\n",
    "    attn = attn.view(batch_size, queryL, sourceL)\n",
    "    # --> batch x sourceL x queryL\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
    "    # --> batch x ndf x queryL\n",
    "    weightedContext = torch.bmm(context, attnT)\n",
    "\n",
    "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "class AddCoords(nn.Module):\n",
    "    \"\"\"\n",
    "    Add Coords to a tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, with_r=False):\n",
    "        super(AddCoords, self).__init__()\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: shape (batch, channel, x_dim, y_dim)\n",
    "        :return: shape (batch, channel+2, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        B, _, x_dim, y_dim = x.size()\n",
    "\n",
    "        # coord calculate\n",
    "        xx_channel = torch.arange(x_dim).repeat(B, 1, y_dim, 1).type_as(x)\n",
    "        yy_cahnnel = torch.arange(y_dim).repeat(B, 1, x_dim, 1).permute(0, 1, 3, 2).type_as(x)\n",
    "        # normalization\n",
    "        xx_channel = xx_channel.float() / (x_dim-1)\n",
    "        yy_cahnnel = yy_cahnnel.float() / (y_dim-1)\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_cahnnel = yy_cahnnel * 2 - 1\n",
    "\n",
    "        ret = torch.cat([x, xx_channel, yy_cahnnel], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(xx_channel ** 2 + yy_cahnnel ** 2)\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class CoordConv(nn.Module):\n",
    "    \"\"\"\n",
    "    CoordConv operation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc, output_nc, with_r=False, use_spect=False, **kwargs):\n",
    "        super(CoordConv, self).__init__()\n",
    "        self.addcoords = AddCoords(with_r=with_r)\n",
    "        input_nc = input_nc + 2\n",
    "        if with_r:\n",
    "            input_nc = input_nc + 1\n",
    "        self.conv = spectral_norm(nn.Conv2d(input_nc, output_nc, **kwargs), use_spect)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.addcoords(x)\n",
    "        ret = self.conv(ret)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Define an Residual block for different types\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc, output_nc, hidden_nc=None, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
    "                 sample_type='none', use_spect=False, use_coord=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        hidden_nc = output_nc if hidden_nc is None else hidden_nc\n",
    "        self.sample = True\n",
    "        if sample_type == 'none':\n",
    "            self.sample = False\n",
    "        elif sample_type == 'up':\n",
    "            output_nc = output_nc * 4\n",
    "            self.pool = nn.PixelShuffle(upscale_factor=2)\n",
    "        elif sample_type == 'down':\n",
    "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        else:\n",
    "            raise NotImplementedError('sample type [%s] is not found' % sample_type)\n",
    "\n",
    "        kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "        kwargs_short = {'kernel_size': 1, 'stride': 1, 'padding': 0}\n",
    "\n",
    "        self.conv1 = coord_conv(input_nc, hidden_nc, use_spect, use_coord, **kwargs)\n",
    "        self.conv2 = coord_conv(hidden_nc, output_nc, use_spect, use_coord, **kwargs)\n",
    "        self.bypass = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs_short)\n",
    "\n",
    "        if type(norm_layer) == type(None):\n",
    "            self.model = nn.Sequential(nonlinearity, self.conv1, nonlinearity, self.conv2,)\n",
    "        else:\n",
    "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, self.conv1, norm_layer(hidden_nc), nonlinearity, self.conv2,)\n",
    "\n",
    "        self.shortcut = nn.Sequential(self.bypass,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sample:\n",
    "            out = self.pool(self.model(x)) + self.pool(self.shortcut(x))\n",
    "        else:\n",
    "            out = self.model(x) + self.shortcut(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResBlockEncoderOptimized(nn.Module):\n",
    "    \"\"\"\n",
    "    Define an Encoder block for the first layer of the discriminator and representation network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(), use_spect=False, use_coord=False):\n",
    "        super(ResBlockEncoderOptimized, self).__init__()\n",
    "\n",
    "        kwargs = {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
    "        kwargs_short = {'kernel_size': 1, 'stride': 1, 'padding': 0}\n",
    "\n",
    "        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)\n",
    "        self.conv2 = coord_conv(output_nc, output_nc, use_spect, use_coord, **kwargs)\n",
    "        self.bypass = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs_short)\n",
    "\n",
    "        if type(norm_layer) == type(None):\n",
    "            self.model = nn.Sequential(self.conv1, nonlinearity, self.conv2, nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        else:\n",
    "            self.model = nn.Sequential(self.conv1, norm_layer(output_nc), nonlinearity, self.conv2, nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.shortcut = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2), self.bypass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x) + self.shortcut(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResBlockDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a decoder block\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc, output_nc, hidden_nc=None, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
    "                 use_spect=False, use_coord=False):\n",
    "        super(ResBlockDecoder, self).__init__()\n",
    "\n",
    "        hidden_nc = output_nc if hidden_nc is None else hidden_nc\n",
    "\n",
    "        self.conv1 = spectral_norm(nn.Conv2d(input_nc, hidden_nc, kernel_size=3, stride=1, padding=1), use_spect)\n",
    "        self.conv2 = spectral_norm(nn.ConvTranspose2d(hidden_nc, output_nc, kernel_size=3, stride=2, padding=1, output_padding=1), use_spect)\n",
    "        self.bypass = spectral_norm(nn.ConvTranspose2d(input_nc, output_nc, kernel_size=3, stride=2, padding=1, output_padding=1), use_spect)\n",
    "\n",
    "        if type(norm_layer) == type(None):\n",
    "            self.model = nn.Sequential(nonlinearity, self.conv1, nonlinearity, self.conv2,)\n",
    "        else:\n",
    "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, self.conv1, norm_layer(hidden_nc), nonlinearity, self.conv2,)\n",
    "\n",
    "        self.shortcut = nn.Sequential(self.bypass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x) + self.shortcut(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Output(nn.Module):\n",
    "    \"\"\"\n",
    "    Define the output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc, output_nc, kernel_size = 3, norm_layer=nn.BatchNorm2d, nonlinearity= nn.LeakyReLU(),\n",
    "                 use_spect=False, use_coord=False):\n",
    "        super(Output, self).__init__()\n",
    "\n",
    "        kwargs = {'kernel_size': kernel_size, 'padding':0, 'bias': True}\n",
    "\n",
    "        self.conv1 = coord_conv(input_nc, output_nc, use_spect, use_coord, **kwargs)\n",
    "\n",
    "        if type(norm_layer) == type(None):\n",
    "            self.model = nn.Sequential(nonlinearity, nn.ReflectionPad2d(int(kernel_size/2)), self.conv1, nn.Tanh())\n",
    "        else:\n",
    "            self.model = nn.Sequential(norm_layer(input_nc), nonlinearity, nn.ReflectionPad2d(int(kernel_size / 2)), self.conv1, nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Auto_Attn(nn.Module):\n",
    "    \"\"\" Short+Long attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, norm_layer=nn.BatchNorm2d):\n",
    "        super(Auto_Attn, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "\n",
    "        self.query_conv = nn.Conv2d(input_nc, input_nc // 4, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.alpha = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.model = ResBlock(int(input_nc*2), input_nc, input_nc, norm_layer=norm_layer, use_spect=True)\n",
    "\n",
    "    def forward(self, x, pre=None, mask=None):\n",
    "        \"\"\"\n",
    "        inputs :\n",
    "            x : input feature maps( B X C X W X H)\n",
    "        returns :\n",
    "            out : self attention value + input feature\n",
    "            attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        B, C, W, H = x.size()\n",
    "        proj_query = self.query_conv(x).view(B, -1, W * H)  # B X (N)X C\n",
    "        proj_key = proj_query  # B X C x (N)\n",
    "\n",
    "        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)  # transpose check\n",
    "        attention = self.softmax(energy)  # BX (N) X (N)\n",
    "        proj_value = x.view(B, -1, W * H)  # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, W, H)\n",
    "\n",
    "        out = self.gamma * out + x\n",
    "\n",
    "        if type(pre) != type(None):\n",
    "            # using long distance attention layer to copy information from valid regions\n",
    "            context_flow = torch.bmm(pre.view(B, -1, W*H), attention.permute(0, 2, 1)).view(B, -1, W, H)\n",
    "            context_flow = self.alpha * (1-mask) * context_flow + (mask) * pre\n",
    "            out = self.model(torch.cat([out, context_flow], dim=1))\n",
    "\n",
    "        return out, attention\n",
    "\n",
    "\n",
    "class ImageTextAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Global attention takes a matrix and a query metrix.\n",
    "    Based on each query vector q, it computes a parameterized convex combination of the matrix\n",
    "    based.\n",
    "    H_1 H_2 H_3 ... H_n\n",
    "      q   q   q       q\n",
    "        |  |   |       |\n",
    "          \\ |   |      /\n",
    "                  .....\n",
    "              \\   |  /\n",
    "                      a\n",
    "    Constructs a unit mapping.\n",
    "    $$(H_1 + H_n, q) => (a)$$\n",
    "    Where H is of `batch x n x dim` and q is of `batch x dim`.\n",
    "    References:\n",
    "    https://github.com/OpenNMT/OpenNMT-py/tree/fc23dfef1ba2f258858b2765d24565266526dc76/onmt/modules\n",
    "    http://www.aclweb.org/anthology/D15-1166\n",
    "\n",
    "    :param idf: image dimension\n",
    "    :param cdf: text dimension\n",
    "    :param multi_peak: use sigmoid when computing word attention\n",
    "    :param pooling: pooling layer type on weightedConext\n",
    "    \"\"\"\n",
    "    def __init__(self, idf, cdf, multi_peak=False, pooling='max'):\n",
    "        super(ImageTextAttention, self).__init__()\n",
    "        self.conv_image = conv1x1(idf, cdf)\n",
    "        self.sm = nn.Softmax()\n",
    "        self.multi_peak = multi_peak\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.pooling = pooling\n",
    "        if self.pooling == 'max':\n",
    "            self.pooling_layer = nn.AdaptiveMaxPool2d(1)\n",
    "        elif self.pooling == 'avg':\n",
    "            self.pooling_layer = nn.AdaptiveAvgPool2d(1)\n",
    "        else:\n",
    "            self.pooling = False\n",
    "\n",
    "    def forward_softmax(self, image, text, mask=None, image_mask=None, inverse_attention=False):\n",
    "        \"\"\"\n",
    "            input: batch x idf x ih x iw (image_L=ihxiw)\n",
    "            context: batch x cdf x text_L\n",
    "        \"\"\"\n",
    "        ih, iw = image.size(2), image.size(3)\n",
    "        image_L = ih * iw\n",
    "        batch_size, text_L = text.size(0), text.size(2)\n",
    "\n",
    "        # --> batch x image_L x idf\n",
    "        image = self.conv_image(image)\n",
    "        image_flat = image.view(batch_size, -1, image_L)\n",
    "        image_flat_T = torch.transpose(image_flat, 1, 2).contiguous()\n",
    "\n",
    "        # Get attention\n",
    "        # (batch x image_L x idf)(batch x idf x text_L)\n",
    "        # -->batch x image_L x text_L\n",
    "        attn = torch.bmm(image_flat_T, text)\n",
    "        if inverse_attention:\n",
    "            attn *= -1\n",
    "\n",
    "        if image_mask is not None:\n",
    "            # in img_mask, 0 is masked, so here we inverse the mask value\n",
    "            image_mask = (1-image_mask).bool()#torch.logical_not(image_mask.bool())\n",
    "            image_mask = image_mask.view(-1, image_L, 1).repeat(1, 1, text_L)\n",
    "\n",
    "            attn.data.masked_fill_(image_mask.data, -float('inf'))\n",
    "\n",
    "        # --> batch*image_L x text_L\n",
    "        attn = attn.view(batch_size*image_L, text_L)\n",
    "        if mask is not None:\n",
    "            # batch_size x text_L --> batch_size*image_L x text_L\n",
    "            mask = mask.repeat(image_L, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "\n",
    "        attn = self.sm(attn)  # Eq. (2)\n",
    "        attn.data.masked_fill_(attn != attn, 0)\n",
    "        # --> batch x image_L x text_L\n",
    "        attn = attn.view(batch_size, image_L, text_L)\n",
    "        # --> batch x text_L x image_L\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # (batch x idf x text_L)(batch x text_L x image_L)\n",
    "        # --> batch x idf x image_L\n",
    "        weightedContext = torch.bmm(text, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        # torch.save(attn.detach(), 'attention_map.pt')\n",
    "\n",
    "        return weightedContext\n",
    "\n",
    "    def forward_sigmoid(self, image, text, mask=None, image_mask=None, inverse_attention=False):\n",
    "        \"\"\"\n",
    "            input: batch x idf x ih x iw (image_L=ihxiw)\n",
    "            context: batch x cdf x text_L\n",
    "        \"\"\"\n",
    "        ih, iw = image.size(2), image.size(3)\n",
    "        image_L = ih * iw\n",
    "        batch_size, text_L = text.size(0), text.size(2)\n",
    "\n",
    "        # --> batch x image_L x idf\n",
    "        image = self.conv_image(image)\n",
    "        image_flat = image.view(batch_size, -1, image_L)\n",
    "        image_flat_T = torch.transpose(image_flat, 1, 2).contiguous()\n",
    "\n",
    "        # Get attention\n",
    "        # (batch x image_L x idf)(batch x idf x text_L)\n",
    "        # -->batch x image_L x text_L\n",
    "        attn = torch.bmm(image_flat_T, text)\n",
    "\n",
    "        # Apply mask\n",
    "        if image_mask is not None:\n",
    "            # in img_mask, 0 is masked, so here we inverse the mask value\n",
    "            image_mask = (1-image_mask).bool()#torch.logical_not(image_mask.bool())\n",
    "            image_mask = image_mask.view(-1, image_L, 1).repeat(1, 1, text_L)\n",
    "\n",
    "            attn.data.masked_fill_(image_mask.data, -float('inf'))\n",
    "\n",
    "        # --> batch*image_L x text_L\n",
    "        attn = attn.view(batch_size*image_L, text_L)\n",
    "        if mask is not None:\n",
    "            # batch_size x text_L --> batch_size*image_L x text_L\n",
    "            mask = mask.repeat(image_L, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "\n",
    "        attn = self.sigmoid(attn)  # Eq. (2)\n",
    "        if inverse_attention:\n",
    "            attn = 1 - attn\n",
    "\n",
    "        attn.data.masked_fill_(attn != attn, 0)\n",
    "        # --> batch x image_L x text_L\n",
    "        attn = attn.view(batch_size, image_L, text_L)\n",
    "        # --> batch x text_L x image_L\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # (batch x idf x text_L)(batch x text_L x image_L)\n",
    "        # --> batch x idf x image_L\n",
    "        weightedContext = torch.bmm(text, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        return weightedContext\n",
    "\n",
    "    def forward(self, image, text, mask=None, image_mask=None, inverse_attention=False):\n",
    "        if self.multi_peak:\n",
    "            weightedContext = self.forward_sigmoid(image, text, mask, image_mask, inverse_attention)\n",
    "        else:\n",
    "            weightedContext = self.forward_softmax(image, text, mask, image_mask, inverse_attention)\n",
    "        if self.pooling is not False:\n",
    "            ih, iw = weightedContext.size(2), weightedContext.size(3)\n",
    "            weightedContext = self.pooling_layer(weightedContext)\n",
    "            weightedContext = weightedContext.repeat(1, 1, ih, iw)\n",
    "\n",
    "        return weightedContext\n",
    "\n",
    "class GlobalAttentionGeneral(nn.Module):\n",
    "    \"\"\"\n",
    "    Global attention takes a matrix and a query metrix.\n",
    "    Based on each query vector q, it computes a parameterized convex combination of the matrix\n",
    "    based.\n",
    "    H_1 H_2 H_3 ... H_n\n",
    "      q   q   q       q\n",
    "        |  |   |       |\n",
    "          \\ |   |      /\n",
    "                  .....\n",
    "              \\   |  /\n",
    "                      a\n",
    "    Constructs a unit mapping.\n",
    "    $$(H_1 + H_n, q) => (a)$$\n",
    "    Where H is of `batch x n x dim` and q is of `batch x dim`.\n",
    "    References:\n",
    "    https://github.com/OpenNMT/OpenNMT-py/tree/fc23dfef1ba2f258858b2765d24565266526dc76/onmt/modules\n",
    "    http://www.aclweb.org/anthology/D15-1166\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idf, cdf):\n",
    "        super(GlobalAttentionGeneral, self).__init__()\n",
    "        self.conv_context = conv1x1(cdf, idf)\n",
    "        self.sm = nn.Softmax()\n",
    "\n",
    "    def forward(self, input, context, mask=None, inverse_attention=False):\n",
    "        \"\"\"\n",
    "            input: batch x idf x ih x iw (queryL=ihxiw)\n",
    "            context: batch x cdf x sourceL\n",
    "        \"\"\"\n",
    "        ih, iw = input.size(2), input.size(3)\n",
    "        queryL = ih * iw\n",
    "        batch_size, sourceL = context.size(0), context.size(2)\n",
    "\n",
    "        # --> batch x queryL x idf\n",
    "        target = input.view(batch_size, -1, queryL)\n",
    "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
    "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
    "        sourceT = context.unsqueeze(3)\n",
    "        # --> batch x idf x sourceL\n",
    "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
    "\n",
    "        # Get attention\n",
    "        # (batch x queryL x idf)(batch x idf x sourceL)\n",
    "        # -->batch x queryL x sourceL\n",
    "        attn = torch.bmm(targetT, sourceT)\n",
    "        if inverse_attention:\n",
    "            attn *= -1\n",
    "        attn = attn.view(batch_size * queryL, sourceL)\n",
    "        if mask is not None:\n",
    "            # batch_size x sourceL --> batch_size*queryL x sourceL\n",
    "            mask = mask.repeat(queryL, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "\n",
    "        attn = self.sm(attn)  # Eq. (2)\n",
    "        # --> batch x queryL x sourceL\n",
    "        attn = attn.view(batch_size, queryL, sourceL)\n",
    "        # --> batch x sourceL x queryL\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # (batch x idf x sourceL)(batch x sourceL x queryL)\n",
    "        # --> batch x idf x queryL\n",
    "        weightedContext = torch.bmm(sourceT, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        return weightedContext\n",
    "\n",
    "# ##################Loss for matching text-image###################\n",
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps))\n",
    "\n",
    "\n",
    "def sent_loss(cnn_code, rnn_code, labels, eps=1e-8, smooth_gama3=10.0):\n",
    "\n",
    "    # --> seq_len x batch_size x nef\n",
    "    if cnn_code.dim() == 2:\n",
    "        cnn_code = cnn_code.unsqueeze(0)\n",
    "        rnn_code = rnn_code.unsqueeze(0)\n",
    "\n",
    "    # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
    "    cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
    "    rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
    "    # scores* / norm*: seq_len x batch_size x batch_size\n",
    "    scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
    "    norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
    "    scores0 = scores0 / norm0.clamp(min=eps) * smooth_gama3\n",
    "\n",
    "    # --> batch_size x batch_size\n",
    "    scores0 = scores0.squeeze(0)\n",
    "\n",
    "    scores1 = scores0.transpose(0, 1)\n",
    "    if labels is not None:\n",
    "        loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
    "        loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
    "    else:\n",
    "        loss0, loss1 = None, None\n",
    "    return loss0 + loss1\n",
    "\n",
    "\n",
    "def words_loss(img_features, words_emb, labels, cap_lens, batch_size,\n",
    "                            smooth_gamma1=5.0, smooth_gamma2=5.0, smooth_gamma3=10.0):\n",
    "    \"\"\"\n",
    "        words_emb(query): batch x nef x seq_len\n",
    "        img_features(context): batch x nef x 17 x 17\n",
    "    \"\"\"\n",
    "\n",
    "    att_maps = []\n",
    "    similarities = []\n",
    "    cap_lens = cap_lens.data.tolist()\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        # Get the i-th text description\n",
    "        words_num = cap_lens[i]\n",
    "        # -> 1 x nef x words_num\n",
    "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
    "        # -> batch_size x nef x words_num\n",
    "        word = word.repeat(batch_size, 1, 1)\n",
    "        # batch x nef x 17*17\n",
    "        context = img_features\n",
    "        \"\"\"\n",
    "            word(query): batch x nef x words_num\n",
    "            context: batch x nef x 17 x 17\n",
    "            weiContext: batch x nef x words_num\n",
    "            attn: batch x words_num x 17 x 17\n",
    "        \"\"\"\n",
    "        weiContext, attn = func_attention(word, context, smooth_gamma1)\n",
    "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
    "        # --> batch_size x words_num x nef\n",
    "        word = word.transpose(1, 2).contiguous()\n",
    "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
    "        # --> batch_size*words_num x nef\n",
    "        word = word.view(batch_size * words_num, -1)\n",
    "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
    "        #\n",
    "        # -->batch_size*words_num\n",
    "        row_sim = cosine_similarity(word, weiContext)\n",
    "        # --> batch_size x words_num\n",
    "        row_sim = row_sim.view(batch_size, words_num)\n",
    "\n",
    "        # Eq. (10)\n",
    "        row_sim.mul_(smooth_gamma2).exp_()\n",
    "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "        row_sim = torch.log(row_sim)\n",
    "\n",
    "        # --> 1 x batch_size\n",
    "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
    "        similarities.append(row_sim)\n",
    "\n",
    "    # batch_size x batch_size\n",
    "    similarities = torch.cat(similarities, 1)\n",
    "\n",
    "    similarities = similarities * smooth_gamma3\n",
    "\n",
    "    similarities1 = similarities.transpose(0, 1)\n",
    "    if labels is not None:\n",
    "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
    "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
    "    else:\n",
    "        loss0, loss1 = None, None\n",
    "    return loss0 + loss1, att_maps\n",
    "\n",
    "class GANHingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANHingeLoss, self).__init__()\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def __call__(self, pos, neg):\n",
    "        hinge_pos = torch.mean(self.activation(1-pos))\n",
    "        hinge_neg = torch.mean(self.activation(1+neg))\n",
    "        d_loss = .5 * hinge_pos + .5 * hinge_neg\n",
    "        g_loss = -torch.mean(neg)\n",
    "\n",
    "        return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc6bbbf-f7d4-4de1-a85a-339e1fb9077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.isTrain = opt.isTrain\n",
    "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
    "        self.loss_names = []\n",
    "        self.log_names = []\n",
    "        self.model_names = []\n",
    "        self.visual_names = []\n",
    "        self.text_names = []\n",
    "        self.value_names = []\n",
    "        self.image_paths = []\n",
    "        self.optimizers = []\n",
    "        self.schedulers = []\n",
    "\n",
    "    def name(self):\n",
    "        return 'BaseModel'\n",
    "\n",
    "    @staticmethod\n",
    "    def modify_options(parser, is_train):\n",
    "        \"\"\"Add new options and rewrite default values for existing options\"\"\"\n",
    "        return parser\n",
    "\n",
    "    def set_input(self, input):\n",
    "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, opt):\n",
    "        \"\"\"Load networks, create schedulers\"\"\"\n",
    "        if self.isTrain:\n",
    "            self.schedulers = [base_function.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
    "        if not self.isTrain or opt.continue_train:\n",
    "            self.load_networks(opt.which_iter, opt.gpu_ids)\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Make models eval mode during test time\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                net = getattr(self, 'net_' + name)\n",
    "                net.eval()\n",
    "\n",
    "    def get_image_paths(self):\n",
    "        \"\"\"Return image paths that are used to load current data\"\"\"\n",
    "        return self.image_paths\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Update learning rate\"\"\"\n",
    "        for scheduler in self.schedulers:\n",
    "            scheduler.step()\n",
    "        lr = self.optimizers[0].param_groups[0]['lr']\n",
    "        print('learning rate=%.7f' % lr)\n",
    "\n",
    "    def get_current_errors(self):\n",
    "        \"\"\"Return training loss\"\"\"\n",
    "        errors_ret = OrderedDict()\n",
    "        for name in self.loss_names:\n",
    "            if isinstance(name, str):\n",
    "                errors_ret[name] = getattr(self, 'loss_' + name).item()\n",
    "\n",
    "        if 'img_truth' in self.visual_names:\n",
    "            truth = getattr(self, 'img_truth')\n",
    "            outputs_names = ['img_out', 'img_g', 'img_rec']\n",
    "            for name in outputs_names:\n",
    "                if name in self.visual_names:\n",
    "                    out = getattr(self, name)\n",
    "                    psnr = util.PSNR(util.tensor2im(out[-1].data), util.tensor2im(truth[-1].data))\n",
    "                    errors_ret['psnr_'+name] = psnr\n",
    "\n",
    "        return errors_ret\n",
    "\n",
    "    def get_current_visuals(self):\n",
    "        \"\"\"Return visualization images\"\"\"\n",
    "        visual_ret = OrderedDict()\n",
    "        for name in self.visual_names:\n",
    "            if isinstance(name, str):\n",
    "                value = getattr(self, name)\n",
    "                if isinstance(value, list):\n",
    "                    visual_ret[name] = util.tensor2im(value[-1].data)\n",
    "                else:\n",
    "                    visual_ret[name] = util.tensor2im(value.data)\n",
    "        return visual_ret\n",
    "\n",
    "    def get_current_text(self):\n",
    "        \"\"\"Return the last image's caption of current batch\"\"\"\n",
    "        text_ret = OrderedDict()\n",
    "        for name in self.text_names:\n",
    "            if isinstance(name, str):\n",
    "                text = getattr(self, name)\n",
    "                if isinstance(text, list):\n",
    "                    text_ret[name] = text[-1] + '\\n'+ self.image_paths[0]\n",
    "                else:\n",
    "                    text_ret[name] = text + '\\n' + self.image_paths[0]\n",
    "        return text_ret\n",
    "\n",
    "    def get_current_dis(self):\n",
    "        \"\"\"Return the distribution of encoder features\"\"\"\n",
    "        dis_ret = OrderedDict()\n",
    "        value = getattr(self, 'distribution')\n",
    "        for i in range(1):\n",
    "            for j, name in enumerate(self.value_names):\n",
    "                if isinstance(name, str):\n",
    "                    dis_ret[name+str(i)] =util.tensor2array(value[i][j].data)\n",
    "\n",
    "        return dis_ret\n",
    "\n",
    "    # save model\n",
    "    def save_networks(self, which_epoch):\n",
    "        \"\"\"Save all the networks to the disk\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n",
    "                save_path = os.path.join(self.save_dir, save_filename)\n",
    "                net = getattr(self, 'net_' + name)\n",
    "                torch.save(net.cpu().state_dict(), save_path)\n",
    "                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "                    net.cuda()\n",
    "\n",
    "    # load models\n",
    "    def load_networks(self, which_epoch, gpu_ids):\n",
    "        \"\"\"Load all the networks from the disk\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                filename = '%s_net_%s.pth' % (which_epoch, name)\n",
    "                path = os.path.join(self.save_dir, filename)\n",
    "                net = getattr(self, 'net_' + name)\n",
    "                pretrained_dict = torch.load(path)\n",
    "                try:\n",
    "                    if len(gpu_ids) != 0:\n",
    "                        net.load_state_dict(pretrained_dict)\n",
    "                    else:\n",
    "                        pretrained_dict_cpu = {key[7:]:value for key, value in pretrained_dict.items()}\n",
    "                        net.load_state_dict(pretrained_dict_cpu)\n",
    "                except:\n",
    "                    model_dict = net.state_dict()\n",
    "                    try:\n",
    "                        pretrained_dict = {k:v for k,v in pretrained_dict.items() if k in model_dict}\n",
    "                        net.load_state_dict(pretrained_dict)\n",
    "                        print('Pretrained network %s has excessive layers; Only loading layers that are used' % name)\n",
    "                    except:\n",
    "                        print('Pretrained network %s has fewer layers; The following are not initialized:' % name)\n",
    "                        not_initialized = set()\n",
    "                        for k, v in pretrained_dict.items():\n",
    "                            if v.size() == model_dict[k].size():\n",
    "                                model_dict[k] = v\n",
    "\n",
    "                        for k, v in model_dict.items():\n",
    "                            if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n",
    "                                not_initialized.add(k.split('.')[0])\n",
    "                        print(sorted(not_initialized))\n",
    "                        net.load_state_dict(model_dict)\n",
    "                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "                    net.cuda()\n",
    "                if not self.isTrain:\n",
    "                    net.eval()\n",
    "\n",
    "    def save_results(self, save_data, score=None, data_name='none', mark=None):\n",
    "        \"\"\"Save the training or testing results to disk\"\"\"\n",
    "        img_paths = self.get_image_paths()\n",
    "\n",
    "        for i in range(save_data.size(0)):\n",
    "            print('process image ...... %s' % img_paths[i])\n",
    "            short_path = ntpath.basename(img_paths[i])  # get image path\n",
    "            name = os.path.splitext(short_path)[0]\n",
    "            if type(score) == type(None):\n",
    "                img_name = '%s_%s.png' % (name, data_name)\n",
    "            else:\n",
    "                # d_score = score[i].mean()\n",
    "                # img_name = '%s_%s_%s.png' % (name, data_name, str(round(d_score.item(), 3)))\n",
    "                if type(mark) == type(None):\n",
    "                    img_name = '%s_%s_%s.png' % (name, data_name, str(score))\n",
    "                else:\n",
    "                    img_name = '%s_%s_%s_%s.png' % (name, data_name, str(score), str(mark))\n",
    "            # save predicted image with discriminator score\n",
    "            util.mkdir(self.opt.results_dir)\n",
    "            img_path = os.path.join(self.opt.results_dir, img_name)\n",
    "            img_numpy = util.tensor2im(save_data[i].data)\n",
    "            util.save_image(img_numpy, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0114da-e6da-4de8-9eb7-5cad5b8208d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# spectral normalization layer to decouple the magnitude of a weight tensor\n",
    "####################################################################################################\n",
    "\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    spectral normalization\n",
    "    code and idea originally from Takeru Miyato's work 'Spectral Normalization for GAN'\n",
    "    https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n",
    "    \"\"\"\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# adversarial loss for different gan mode\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define different GAN objectives.\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        \"\"\" Initialize the GANLoss class.\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_fake_label (bool) - - label of a fake image\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'hinge':\n",
    "            self.loss = nn.ReLU()\n",
    "        elif gan_mode == 'wgangp':\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real, is_disc=False):\n",
    "        \"\"\" Calculate loss given Discriminator's output and grount truth labels.\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            labels = (self.real_label if target_is_real else self.fake_label).expand_as(prediction).type_as(prediction)\n",
    "            loss = self.loss(prediction, labels)\n",
    "        elif self.gan_mode in ['hinge', 'wgangp']:\n",
    "            if is_disc:\n",
    "                if target_is_real:\n",
    "                    prediction = -prediction\n",
    "                if self.gan_mode == 'hinge':\n",
    "                    loss = self.loss(1 + prediction).mean()\n",
    "                elif self.gan_mode == 'wgangp':\n",
    "                    loss = prediction.mean()\n",
    "            else:\n",
    "                loss = -prediction.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def cal_gradient_penalty(netD, real_data, fake_data, type='mixed', constant=1.0, lambda_gp=10.0):\n",
    "    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n",
    "    Arguments:\n",
    "        netD (network)              -- discriminator network\n",
    "        real_data (tensor array)    -- real images\n",
    "        fake_data (tensor array)    -- generated images from the generator\n",
    "        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n",
    "        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n",
    "        lambda_gp (float)           -- weight for this loss\n",
    "    Returns the gradient penalty loss\n",
    "    \"\"\"\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
    "            alpha = alpha.type_as(real_data)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).type_as(real_data),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# neural style transform loss from neural_style_tutorial of pytorch\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def ContentLoss(input, target):\n",
    "    target = target.detach()\n",
    "    loss = F.l1_loss(input, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def GramMatrix(input):\n",
    "    s = input.size()\n",
    "    features = input.view(s[0], s[1], s[2]*s[3])\n",
    "    features_t = torch.transpose(features, 1, 2)\n",
    "    G = torch.bmm(features, features_t).div(s[1]*s[2]*s[3])\n",
    "    return G\n",
    "\n",
    "\n",
    "def StyleLoss(input, target):\n",
    "    target = GramMatrix(target).detach()\n",
    "    input = GramMatrix(input)\n",
    "    loss = F.l1_loss(input, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def img_crop(input, size=224):\n",
    "    input_cropped = F.upsample(input, size=(size, size), mode='bilinear', align_corners=True)\n",
    "    return input_cropped\n",
    "\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = mean.view(-1, 1, 1)\n",
    "        self.std = std.view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input-self.mean) / self.std\n",
    "\n",
    "\n",
    "class get_features(nn.Module):\n",
    "    def __init__(self, cnn):\n",
    "        super(get_features, self).__init__()\n",
    "\n",
    "        vgg = copy.deepcopy(cnn)\n",
    "\n",
    "        self.conv1 = nn.Sequential(vgg[0], vgg[1], vgg[2], vgg[3], vgg[4])\n",
    "        self.conv2 = nn.Sequential(vgg[5], vgg[6], vgg[7], vgg[8], vgg[9])\n",
    "        self.conv3 = nn.Sequential(vgg[10], vgg[11], vgg[12], vgg[13], vgg[14], vgg[15], vgg[16])\n",
    "        self.conv4 = nn.Sequential(vgg[17], vgg[18], vgg[19], vgg[20], vgg[21], vgg[22], vgg[23])\n",
    "        self.conv5 = nn.Sequential(vgg[24], vgg[25], vgg[26], vgg[27], vgg[28], vgg[29], vgg[30])\n",
    "\n",
    "    def forward(self, input, layers):\n",
    "        input = img_crop(input)\n",
    "        output = []\n",
    "        for i in range(1, layers):\n",
    "            layer = getattr(self, 'conv'+str(i))\n",
    "            input = layer(input)\n",
    "            output.append(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83f7c4c-b8bd-417b-863a-047b60de8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "# Network function\n",
    "##############################################################################################################\n",
    "def define_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[]):\n",
    "\n",
    "    net = ResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord)\n",
    "\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_textual_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[], image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "    net = TextualResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord, image_dim, text_dim, multi_peak, pool_attention)\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_att_textual_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[], image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "    net = AttTextualResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord, image_dim, text_dim, multi_peak, pool_attention)\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_contract_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[], image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "    net = ContrastResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord, image_dim, text_dim, multi_peak, pool_attention)\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_pos_textual_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[], image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "    net = PosAttTextualResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord, image_dim, text_dim, multi_peak, pool_attention)\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_word_attn_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[], image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "    net = WordAttnEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord, image_dim, text_dim, multi_peak, pool_attention)\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_constraint_e(input_nc=3, ngf=64, z_nc=512, img_f=512, L=6, layers=5, norm='none', activation='ReLU', use_spect=True,\n",
    "             use_coord=False, init_type='orthogonal', gpu_ids=[], image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "    net = ConstraintResEncoder(input_nc, ngf, z_nc, img_f, L, layers, norm, activation, use_spect, use_coord, image_dim, text_dim, multi_peak, pool_attention)\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_g(output_nc=3, ngf=64, z_nc=512, img_f=512, L=1, layers=5, norm='instance', activation='ReLU', output_scale=1,\n",
    "             use_spect=True, use_coord=False, use_attn=True, init_type='orthogonal', gpu_ids=[]):\n",
    "\n",
    "    net = ResGenerator(output_nc, ngf, z_nc, img_f, L, layers, norm, activation, output_scale, use_spect, use_coord, use_attn)\n",
    "\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_constrast_g(output_nc=3, ngf=64, z_nc=512, img_f=512, L=1, layers=5, norm='instance', activation='ReLU', output_scale=1,\n",
    "             use_spect=True, use_coord=False, use_attn=True, init_type='orthogonal', gpu_ids=[]):\n",
    "\n",
    "    net = ContrastResGenerator(output_nc, ngf, z_nc, img_f, L, layers, norm, activation, output_scale, use_spect, use_coord, use_attn)\n",
    "\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "\n",
    "def define_textual_g(output_nc=3, f_text_dim=384, ngf=64, z_nc=512, img_f=512, L=1, layers=5, norm='instance', activation='ReLU', output_scale=1,\n",
    "             use_spect=True, use_coord=False, use_attn=True, init_type='orthogonal', gpu_ids=[]):\n",
    "\n",
    "    net = TextualResGenerator(output_nc, f_text_dim, ngf, z_nc, img_f, L, layers, norm, activation, output_scale, use_spect, use_coord, use_attn)\n",
    "\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_hidden_textual_g(output_nc=3, f_text_dim=384, ngf=64, z_nc=512, img_f=512, L=1, layers=5, norm='instance', activation='ReLU', output_scale=1,\n",
    "             use_spect=True, use_coord=False, use_attn=True, init_type='orthogonal', gpu_ids=[]):\n",
    "\n",
    "    net = HiddenResGenerator(output_nc, f_text_dim, ngf, z_nc, img_f, L, layers, norm, activation, output_scale, use_spect, use_coord, use_attn)\n",
    "\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_d(input_nc=3, ndf=64, img_f=512, layers=6, norm='none', activation='LeakyReLU', use_spect=True, use_coord=False,\n",
    "             use_attn=True,  model_type='ResDis', init_type='orthogonal', gpu_ids=[]):\n",
    "\n",
    "    if model_type == 'ResDis':\n",
    "        net = ResDiscriminator(input_nc, ndf, img_f, layers, norm, activation, use_spect, use_coord, use_attn)\n",
    "    elif model_type == 'PatchDis':\n",
    "        net = SNPatchDiscriminator(input_nc, ndf, img_f, layers, norm, activation, use_spect, use_coord, use_attn)\n",
    "\n",
    "    return init_net(net, init_type, activation, gpu_ids)\n",
    "\n",
    "def define_textual_attention(image_dim, text_dim, multi_peak=True, init_type='orthogonal',  gpu_ids=[]):\n",
    "    net = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak)\n",
    "    return init_net(net, init_type, gpu_ids=gpu_ids)\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "# Network structure\n",
    "#############################################################################################################\n",
    "class ResEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=64, z_nc=128, img_f=1024, L=6, layers=6, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False):\n",
    "        super(ResEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 1), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        self.posterior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        if type(img_c) != type(None):\n",
    "            distribution = self.two_paths(out)\n",
    "            return distribution, feature\n",
    "        else:\n",
    "            distribution = self.one_path(out)\n",
    "            return distribution, feature\n",
    "\n",
    "    def one_path(self, f_in):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        f_m = f_in\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            f_m = infer_prior(f_m)\n",
    "\n",
    "        # get distribution\n",
    "        o = self.prior(f_m)\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    def two_paths(self, f_in):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        distributions = []\n",
    "\n",
    "        # get distribution\n",
    "        o = self.posterior(f_c)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution = self.one_path(f_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "\n",
    "        return distributions\n",
    "\n",
    "class TextualResEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=32, z_nc=256, img_f=256, L=6, layers=5, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False, image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "        super(TextualResEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "        self.word_attention = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak, pooling=pool_attention)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 2), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        # For textual, only change input and hidden dimension, z_nc is set when called.\n",
    "        self.posterior = ResBlock(2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior =     ResBlock(2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, sentence_embedding, word_embeddings, text_mask, image_mask, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param sentence_embedding: the sentence embedding of I\n",
    "        :param word_embeddings: word embedding of I\n",
    "        :param text_mask: mask of word sequence of word_embeddings\n",
    "        :param image_mask: mask of Im and Ic, need to scale if apply to fm\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        :return text_feature: word and sentence features\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        image_mask = task.scale_img(image_mask, size=[feature[-1].size(2), feature[-1].size(3)])\n",
    "        if image_mask.size(1) == 3:\n",
    "            image_mask = image_mask.chunk(3, dim=1)[0]\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            # adapt to word embedding, compute weighted word embedding with fm separately\n",
    "            f_m_g, f_m_rec = feature[-1].chunk(2)\n",
    "            img_mask_g = image_mask\n",
    "            img_mask_rec = 1 - img_mask_g\n",
    "            weighted_word_embedding_rec = self.word_attention(\n",
    "                        f_m_rec, word_embeddings, mask=text_mask, image_mask=img_mask_rec, inverse_attention=False)\n",
    "            weighted_word_embedding_g = self.word_attention(\n",
    "                        f_m_g, word_embeddings, mask=text_mask, image_mask=img_mask_g, inverse_attention=True)\n",
    "\n",
    "            weighted_word_embedding =  torch.cat([weighted_word_embedding_g, weighted_word_embedding_rec])\n",
    "            distribution, f_text = self.two_paths(out, sentence_embedding, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, f_text\n",
    "        else:\n",
    "            # adapt to word embedding, compute weighted word embedding with fm of one path\n",
    "            f_m = feature[-1]\n",
    "            weighted_word_embedding = self.word_attention(\n",
    "                f_m, word_embeddings, mask=text_mask, image_mask=image_mask, inverse_attention=True)\n",
    "\n",
    "            distribution, f_text = self.one_path(out, sentence_embedding, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, f_text\n",
    "\n",
    "    def one_path(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        # TOTEST: adapt to word embedding, compute distribution with word embedding.\n",
    "        f_m = f_in\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            f_m = infer_prior(f_m)\n",
    "\n",
    "        # get distribution\n",
    "        # use sentence embedding here\n",
    "        ix, iw = f_m.size(2), f_m.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_text = torch.cat([sentence_embedding_replication, weighted_word_embedding], dim=1)\n",
    "\n",
    "        o = self.prior(f_text)\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution, f_text\n",
    "\n",
    "    def two_paths(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        weighted_word_embedding_m, weighted_word_embedding_c = weighted_word_embedding.chunk(2)\n",
    "        distributions = []\n",
    "\n",
    "        # get distribution\n",
    "        # use text embedding here\n",
    "        ix, iw = f_c.size(2), f_c.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "\n",
    "        f_text_c = torch.cat([sentence_embedding_replication, weighted_word_embedding_c], dim=1)\n",
    "        o = self.posterior(f_text_c)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "\n",
    "        distribution, f_text_m = self.one_path(f_m, sentence_embedding, weighted_word_embedding_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "\n",
    "        return distributions, torch.cat([f_text_m, f_text_c], dim=0)\n",
    "\n",
    "class AttTextualResEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Attentive Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param image_dim: num of image feature maps\n",
    "    :param text_dim: num of text embedding dimension\n",
    "    :param multi_peak: use sigmoid in text attention if set to True\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=32, z_nc=256, img_f=256, L=6, layers=5, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False, image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "        super(AttTextualResEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "        self.word_attention = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak, pooling=pool_attention)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 2), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        # For textual, only change input and hidden dimension, z_nc is set when called.\n",
    "        self.posterior = ResBlock(ngf * mult + 2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior =     ResBlock(ngf * mult + 2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, sentence_embedding, word_embeddings, text_mask, image_mask, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param sentence_embedding: the sentence embedding of I\n",
    "        :param word_embeddings: word embedding of I\n",
    "        :param text_mask: mask of word sequence of word_embeddings\n",
    "        :param image_mask: mask of Im and Ic, need to scale if apply to fm\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        :return text_feature: word and sentence features\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        image_mask = task.scale_img(image_mask, size=[feature[-1].size(2), feature[-1].size(3)])\n",
    "        if image_mask.size(1) == 3:\n",
    "            image_mask = image_mask.chunk(3, dim=1)[0]\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            # adapt to word embedding, compute weighted word embedding with fm separately\n",
    "            f_m_g, f_m_rec = feature[-1].chunk(2)\n",
    "            img_mask_g = image_mask\n",
    "            img_mask_rec = 1 - img_mask_g\n",
    "            weighted_word_embedding_rec = self.word_attention(\n",
    "                        f_m_rec, word_embeddings, mask=text_mask, image_mask=img_mask_rec, inverse_attention=False)\n",
    "            weighted_word_embedding_g = self.word_attention(\n",
    "                        f_m_g, word_embeddings, mask=text_mask, image_mask=img_mask_g, inverse_attention=True)\n",
    "\n",
    "            weighted_word_embedding =  torch.cat([weighted_word_embedding_g, weighted_word_embedding_rec])\n",
    "            distribution, f_text = self.two_paths(out, sentence_embedding, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, f_text\n",
    "        else:\n",
    "            # adapt to word embedding, compute weighted word embedding with fm of one path\n",
    "            f_m = feature[-1]\n",
    "            weighted_word_embedding = self.word_attention(\n",
    "                f_m, word_embeddings, mask=text_mask, image_mask=image_mask, inverse_attention=True)\n",
    "\n",
    "            distribution, f_m_text = self.one_path(out, sentence_embedding, weighted_word_embedding)\n",
    "            f_text = torch.cat([f_m_text, weighted_word_embedding], dim=1)\n",
    "            return distribution, feature, f_text\n",
    "\n",
    "    def one_path(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        f_m = f_in\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            f_m = infer_prior(f_m)\n",
    "\n",
    "        # get distribution\n",
    "        # use sentence embedding here\n",
    "        ix, iw = f_m.size(2), f_m.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_m_sent = torch.cat([f_m, sentence_embedding_replication], dim=1)\n",
    "        f_m_text = torch.cat([f_m_sent, weighted_word_embedding], dim=1)\n",
    "\n",
    "        o = self.prior(f_m_text)\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution, f_m_sent\n",
    "\n",
    "    def two_paths(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        weighted_word_embedding_m, weighted_word_embedding_c = weighted_word_embedding.chunk(2)\n",
    "        distributions = []\n",
    "\n",
    "        # get distribution\n",
    "        # use text embedding here\n",
    "        ix, iw = f_c.size(2), f_c.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_c_sent = torch.cat([f_c, sentence_embedding_replication], dim=1)\n",
    "        f_c_text = torch.cat([f_c_sent, weighted_word_embedding_c], dim=1)\n",
    "        o = self.posterior(f_c_text)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "\n",
    "        distribution, f_m_sent = self.one_path(f_m, sentence_embedding, weighted_word_embedding_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "\n",
    "        f_m_text = torch.cat([f_m_sent, weighted_word_embedding_m], dim=1)\n",
    "        # TODO: rm weighted_word_emb_c for consis generation\n",
    "        f_c_text = torch.cat([f_m_sent, weighted_word_embedding_c], dim=1)\n",
    "        return distributions, torch.cat([f_m_text, f_c_text], dim=0)\n",
    "\n",
    "class ContrastResEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param image_dim: num of image feature maps\n",
    "    :param text_dim: num of text embedding dimension\n",
    "    :param multi_peak: use sigmoid in text attention if set to True\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=32, z_nc=256, img_f=256, L=6, layers=5, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False, image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "        super(ContrastResEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "        self.word_attention = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak, pooling=pool_attention)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 2), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        # For textual, only change input and hidden dimension, z_nc is set when called.\n",
    "        self.posterior = ResBlock(ngf * mult, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior     = ResBlock(text_dim + ngf * mult, 2*z_nc, 2*ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, sentence_embedding, word_embeddings, text_mask, image_mask, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param sentence_embedding: the sentence embedding of I\n",
    "        :param word_embeddings: word embedding of I\n",
    "        :param text_mask: mask of word sequence of word_embeddings\n",
    "        :param image_mask: mask of Im and Ic, need to scale if apply to fm\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        :return text_feature: word and sentence features\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        image_mask = task.scale_img(image_mask, size=[feature[-1].size(2), feature[-1].size(3)])\n",
    "        if image_mask.size(1) == 3:\n",
    "            image_mask = image_mask.chunk(3, dim=1)[0]\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            # adapt to word embedding, compute weighted word embedding with fm separately\n",
    "            f_m_g, f_m_rec = feature[-1].chunk(2)\n",
    "            img_mask_g = image_mask\n",
    "            img_mask_rec = 1 - img_mask_g\n",
    "            weighted_word_embedding_rec = self.word_attention(\n",
    "                        f_m_rec, word_embeddings, mask=text_mask, image_mask=img_mask_rec, inverse_attention=False)\n",
    "            weighted_word_embedding_g = self.word_attention(\n",
    "                        f_m_g, word_embeddings, mask=text_mask, image_mask=img_mask_g, inverse_attention=True)\n",
    "\n",
    "            weighted_word_embedding =  torch.cat([weighted_word_embedding_g, weighted_word_embedding_rec])\n",
    "            distribution, h_word = self.two_paths(out, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, h_word\n",
    "        else:\n",
    "            # adapt to word embedding, compute weighted word embedding with fm of one path\n",
    "            f_m = feature[-1]\n",
    "            weighted_word_embedding = self.word_attention(\n",
    "                f_m, word_embeddings, mask=text_mask, image_mask=image_mask, inverse_attention=True)\n",
    "\n",
    "            distribution, h_word = self.one_path(weighted_word_embedding, f_m)\n",
    "            return distribution, feature, h_word\n",
    "\n",
    "    def one_path(self, weighted_word_embedding, v_h):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        h_word = weighted_word_embedding\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            h_word = infer_prior(h_word)\n",
    "\n",
    "        # get distribution\n",
    "        o = self.prior(torch.cat([h_word,v_h], dim=1))\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution, h_word\n",
    "\n",
    "    def two_paths(self, f_in, weighted_word_embedding):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        # use text embedding here\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        weighted_word_embedding_m, weighted_word_embedding_c = weighted_word_embedding.chunk(2)\n",
    "\n",
    "        h_word_c = weighted_word_embedding_c\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            h_word_c = infer_prior(h_word_c)\n",
    "\n",
    "        # get distribution\n",
    "        distributions = []\n",
    "        o = self.posterior(f_c)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution, h_word = self.one_path(weighted_word_embedding_m, f_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "\n",
    "        return distributions, torch.cat([h_word, h_word_c], dim=0)\n",
    "\n",
    "\n",
    "class PosAttTextualResEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Positive Attentive Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param image_dim: num of image feature maps\n",
    "    :param text_dim: num of text embedding dimension\n",
    "    :param multi_peak: use sigmoid in text attention if set to True\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=32, z_nc=256, img_f=256, L=6, layers=5, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False, image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "        super(PosAttTextualResEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "        self.word_attention = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak, pooling=pool_attention)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 2), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        # For textual, only change input and hidden dimension, z_nc is set when called.\n",
    "        self.posterior = ResBlock(ngf * mult + 2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior =     ResBlock(ngf * mult + 2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, sentence_embedding, word_embeddings, text_mask, image_mask, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param sentence_embedding: the sentence embedding of I\n",
    "        :param word_embeddings: word embedding of I\n",
    "        :param text_mask: mask of word sequence of word_embeddings\n",
    "        :param image_mask: mask of Im and Ic, need to scale if apply to fm\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        :return text_feature: word and sentence features\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        image_mask = task.scale_img(image_mask, size=[feature[-1].size(2), feature[-1].size(3)])\n",
    "        if image_mask.size(1) == 3:\n",
    "            image_mask = image_mask.chunk(3, dim=1)[0]\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            # adapt to word embedding, compute weighted word embedding with fm separately\n",
    "            f_m_g, f_m_rec = feature[-1].chunk(2)\n",
    "            img_mask_g = image_mask\n",
    "            img_mask_rec = 1 - img_mask_g\n",
    "            weighted_word_embedding_rec = self.word_attention(\n",
    "                        f_m_rec, word_embeddings, mask=text_mask, image_mask=img_mask_rec, inverse_attention=True)\n",
    "            weighted_word_embedding_g = self.word_attention(\n",
    "                        f_m_g, word_embeddings, mask=text_mask, image_mask=img_mask_g, inverse_attention=False)\n",
    "\n",
    "            weighted_word_embedding =  torch.cat([weighted_word_embedding_g, weighted_word_embedding_rec])\n",
    "            distribution, f_text = self.two_paths(out, sentence_embedding, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, f_text\n",
    "        else:\n",
    "            # adapt to word embedding, compute weighted word embedding with fm of one path\n",
    "            f_m = feature[-1]\n",
    "            weighted_word_embedding = self.word_attention(\n",
    "                f_m, word_embeddings, mask=text_mask, image_mask=image_mask, inverse_attention=False)\n",
    "\n",
    "            distribution, f_m_text = self.one_path(out, sentence_embedding, weighted_word_embedding)\n",
    "            f_text = torch.cat([f_m_text, weighted_word_embedding], dim=1)\n",
    "            return distribution, feature, f_text\n",
    "\n",
    "    def one_path(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        # TOTEST: adapt to word embedding, compute distribution with word embedding.\n",
    "        f_m = f_in\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            f_m = infer_prior(f_m)\n",
    "\n",
    "        # get distribution\n",
    "        # use sentence embedding here\n",
    "        ix, iw = f_m.size(2), f_m.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_m_sent = torch.cat([f_m, sentence_embedding_replication], dim=1)\n",
    "        f_m_text = torch.cat([f_m_sent, weighted_word_embedding], dim=1)\n",
    "\n",
    "        o = self.prior(f_m_text)\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution, f_m_sent\n",
    "\n",
    "    def two_paths(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        weighted_word_embedding_m, weighted_word_embedding_c = weighted_word_embedding.chunk(2)\n",
    "        distributions = []\n",
    "\n",
    "        # get distribution\n",
    "        # use text embedding here\n",
    "        ix, iw = f_c.size(2), f_c.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_c_sent = torch.cat([f_c, sentence_embedding_replication], dim=1)\n",
    "        f_c_text = torch.cat([f_c_sent, weighted_word_embedding_c], dim=1)\n",
    "        o = self.posterior(f_c_text)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "\n",
    "        distribution, f_m_sent = self.one_path(f_m, sentence_embedding, weighted_word_embedding_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "\n",
    "        f_m_text = torch.cat([f_m_sent, weighted_word_embedding_m], dim=1)\n",
    "        # TODO: rm weighted_word_emb_c for consis generation\n",
    "        f_c_text = torch.cat([f_m_sent, weighted_word_embedding_c], dim=1)\n",
    "        return distributions, torch.cat([f_m_text, f_c_text], dim=0)\n",
    "\n",
    "\n",
    "class WordAttnEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    WordAttn Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param image_dim: num of image feature maps\n",
    "    :param text_dim: num of text embedding dimension\n",
    "    :param multi_peak: use sigmoid in text attention if set to True\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=32, z_nc=256, img_f=256, L=6, layers=5, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False, image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "        super(WordAttnEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "        self.word_attention = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak, pooling=pool_attention)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 2), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        # For textual, only change input and hidden dimension, z_nc is set when called.\n",
    "        self.posterior = ResBlock(ngf * mult + text_dim, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior =     ResBlock(ngf * mult + text_dim, 2*z_nc, ngf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, word_embeddings, text_mask, image_mask, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param word_embeddings: word embedding of I\n",
    "        :param text_mask: mask of word sequence of word_embeddings\n",
    "        :param image_mask: mask of Im and Ic, need to scale if apply to fm\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        :return text_feature: word and sentence features\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        image_mask = task.scale_img(image_mask, size=[feature[-1].size(2), feature[-1].size(3)])\n",
    "        if image_mask.size(1) == 3:\n",
    "            image_mask = image_mask.chunk(3, dim=1)[0]\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            # adapt to word embedding, compute weighted word embedding with fm separately\n",
    "            f_m_g, f_m_rec = feature[-1].chunk(2)\n",
    "            img_mask_g = image_mask\n",
    "            img_mask_rec = 1 - img_mask_g\n",
    "            weighted_word_embedding_rec = self.word_attention(\n",
    "                        f_m_rec, word_embeddings, mask=text_mask, image_mask=img_mask_rec, inverse_attention=False)\n",
    "            weighted_word_embedding_g = self.word_attention(\n",
    "                        f_m_g, word_embeddings, mask=text_mask, image_mask=img_mask_g, inverse_attention=True)\n",
    "\n",
    "            weighted_word_embedding =  torch.cat([weighted_word_embedding_g, weighted_word_embedding_rec])\n",
    "            distribution, f_text = self.two_paths(out, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, f_text\n",
    "        else:\n",
    "            # adapt to word embedding, compute weighted word embedding with fm of one path\n",
    "            f_m = feature[-1]\n",
    "            weighted_word_embedding = self.word_attention(\n",
    "                f_m, word_embeddings, mask=text_mask, image_mask=image_mask, inverse_attention=True)\n",
    "\n",
    "            distribution = self.one_path(out, weighted_word_embedding)\n",
    "            f_text = weighted_word_embedding\n",
    "            return distribution, feature, f_text\n",
    "\n",
    "    def one_path(self, f_in, weighted_word_embedding):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        # TOTEST: adapt to word embedding, compute distribution with word embedding.\n",
    "        f_m = f_in\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            f_m = infer_prior(f_m)\n",
    "\n",
    "        # get distribution\n",
    "        # use sentence embedding here\n",
    "        f_m_text = torch.cat([f_m, weighted_word_embedding], dim=1)\n",
    "\n",
    "        o = self.prior(f_m_text)\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    def two_paths(self, f_in, weighted_word_embedding):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        weighted_word_embedding_m, weighted_word_embedding_c = weighted_word_embedding.chunk(2)\n",
    "        distributions = []\n",
    "\n",
    "        # get distribution\n",
    "        # use text embedding here\n",
    "        f_c_text = torch.cat([f_c, weighted_word_embedding_c], dim=1)\n",
    "        o = self.posterior(f_c_text)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "\n",
    "        distribution = self.one_path(f_m, weighted_word_embedding_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "\n",
    "        f_m_text = weighted_word_embedding_m\n",
    "        f_c_text = weighted_word_embedding_c\n",
    "        return distributions, torch.cat([f_m_text, f_c_text], dim=0)\n",
    "\n",
    "\n",
    "class ConstraintResEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Constraint Encoder Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param image_dim: num of image feature maps\n",
    "    :param text_dim: num of text embedding dimension\n",
    "    :param multi_peak: use sigmoid in text attention if set to True\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ngf=32, z_nc=256, img_f=256, L=6, layers=5, norm='none', activation='ReLU',\n",
    "                 use_spect=True, use_coord=False, image_dim=256, text_dim=256, multi_peak=True, pool_attention='max'):\n",
    "        super(ConstraintResEncoder, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.z_nc = z_nc\n",
    "        self.L = L\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ngf, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "        self.word_attention = ImageTextAttention(idf=image_dim, cdf=text_dim, multi_peak=multi_peak, pooling=pool_attention)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers-1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 2), img_f // ngf)\n",
    "            block = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        # inference part\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior' + str(i), block)\n",
    "\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf *mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'infer_prior_word' + str(i), block)\n",
    "\n",
    "        # For textual, only change input and hidden dimension, z_nc is set when called.\n",
    "        self.posterior = ResBlock(ngf * mult + 2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.prior =     ResBlock(ngf * mult + 2*text_dim, 2*z_nc, ngf * mult * 2, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "    def forward(self, img_m, sentence_embedding, word_embeddings, text_mask, image_mask, img_c=None):\n",
    "        \"\"\"\n",
    "        :param img_m: image with mask regions I_m\n",
    "        :param sentence_embedding: the sentence embedding of I\n",
    "        :param word_embeddings: word embedding of I\n",
    "        :param text_mask: mask of word sequence of word_embeddings\n",
    "        :param image_mask: mask of Im and Ic, need to scale if apply to fm\n",
    "        :param img_c: complement of I_m, the mask regions\n",
    "        :return distribution: distribution of mask regions, for training we have two paths, testing one path\n",
    "        :return feature: the conditional feature f_m, and the previous f_pre for auto context attention\n",
    "        :return text_feature: word and sentence features\n",
    "        \"\"\"\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            img = torch.cat([img_m, img_c], dim=0)\n",
    "        else:\n",
    "            img = img_m\n",
    "\n",
    "        # encoder part\n",
    "        out = self.block0(img)\n",
    "        feature = [out]\n",
    "        for i in range(self.layers-1):\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "            feature.append(out)\n",
    "\n",
    "        # infer part\n",
    "        # during the training, we have two paths, during the testing, we only have one paths\n",
    "        image_mask = task.scale_img(image_mask, size=[feature[-1].size(2), feature[-1].size(3)])\n",
    "        if image_mask.size(1) == 3:\n",
    "            image_mask = image_mask.chunk(3, dim=1)[0]\n",
    "\n",
    "        if type(img_c) != type(None):\n",
    "            # During training\n",
    "            f_m_g, f_m_rec = feature[-1].chunk(2)\n",
    "            img_mask_g = image_mask\n",
    "            img_mask_rec = 1 - img_mask_g\n",
    "            weighted_word_embedding_rec = self.word_attention(\n",
    "                        f_m_rec, word_embeddings, mask=text_mask, image_mask=img_mask_rec, inverse_attention=False)\n",
    "            weighted_word_embedding_g = self.word_attention(\n",
    "                        f_m_g, word_embeddings, mask=text_mask, image_mask=img_mask_g, inverse_attention=True)\n",
    "\n",
    "            weighted_word_embedding =  torch.cat([weighted_word_embedding_g, weighted_word_embedding_rec])\n",
    "            distribution, f_text, dual_word_embedding = self.two_paths(out, sentence_embedding, weighted_word_embedding)\n",
    "\n",
    "            return distribution, feature, f_text, dual_word_embedding\n",
    "        else:\n",
    "            # During test\n",
    "            f_m = feature[-1]\n",
    "            weighted_word_embedding = self.word_attention(\n",
    "                f_m, word_embeddings, mask=text_mask, image_mask=image_mask, inverse_attention=True)\n",
    "\n",
    "            distribution, f_m_text, infered_word_embedding = self.one_path(out, sentence_embedding, weighted_word_embedding)\n",
    "            f_text = torch.cat([f_m_text, weighted_word_embedding], dim=1)\n",
    "            return distribution, feature, f_text\n",
    "\n",
    "    def one_path(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"one path for baseline training or testing\"\"\"\n",
    "        # TOTEST: adapt to word embedding, compute distribution with word embedding.\n",
    "        f_m = f_in\n",
    "        distribution = []\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior = getattr(self, 'infer_prior' + str(i))\n",
    "            f_m = infer_prior(f_m)\n",
    "\n",
    "        # infer state\n",
    "        for i in range(self.L):\n",
    "            infer_prior_word = getattr(self, 'infer_prior_word' + str(i))\n",
    "            infered_word_embedding = infer_prior_word(weighted_word_embedding)\n",
    "\n",
    "        # get distribution\n",
    "        # use sentence embedding here\n",
    "        ix, iw = f_m.size(2), f_m.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_m_sent = torch.cat([f_m, sentence_embedding_replication], dim=1)\n",
    "        f_m_text = torch.cat([f_m_sent, infered_word_embedding], dim=1)\n",
    "\n",
    "        o = self.prior(f_m_text)\n",
    "        q_mu, q_std = torch.split(o, self.z_nc, dim=1)\n",
    "        distribution.append([q_mu, F.softplus(q_std)])\n",
    "\n",
    "        return distribution, f_m_sent, infered_word_embedding\n",
    "\n",
    "    def two_paths(self, f_in, sentence_embedding, weighted_word_embedding):\n",
    "        \"\"\"two paths for the training\"\"\"\n",
    "        f_m, f_c = f_in.chunk(2)\n",
    "        weighted_word_embedding_m, weighted_word_embedding_c = weighted_word_embedding.chunk(2)\n",
    "        distributions = []\n",
    "\n",
    "        # get distribution\n",
    "        # use text embedding here\n",
    "        ix, iw = f_c.size(2), f_c.size(3)\n",
    "        sentence_dim = sentence_embedding.size(1)\n",
    "        sentence_embedding_replication = sentence_embedding.view(-1, sentence_dim, 1, 1).repeat(1, 1, ix, iw)\n",
    "        f_c_sent = torch.cat([f_c, sentence_embedding_replication], dim=1)\n",
    "        f_c_text = torch.cat([f_c_sent, weighted_word_embedding_c], dim=1)\n",
    "        o = self.posterior(f_c_text)\n",
    "        p_mu, p_std = torch.split(o, self.z_nc, dim=1)\n",
    "\n",
    "        distribution, f_m_sent, infered_word_embedding = self.one_path(f_m, sentence_embedding, weighted_word_embedding_m)\n",
    "        distributions.append([p_mu, F.softplus(p_std), distribution[0][0], distribution[0][1]])\n",
    "        dual_word_embedding = torch.cat([infered_word_embedding, weighted_word_embedding_c], dim=0)\n",
    "\n",
    "        f_m_text = torch.cat([f_m_sent, infered_word_embedding], dim=1)\n",
    "        # TODO: evaluate wether to replace infered to weighted_c\n",
    "        f_c_text = torch.cat([f_m_sent, infered_word_embedding], dim=1)\n",
    "\n",
    "        return distributions, torch.cat([f_m_text, f_c_text], dim=0), dual_word_embedding\n",
    "\n",
    "class ResGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Generator Network\n",
    "    :param output_nc: number of channels in output\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param output_scale: Different output scales\n",
    "    \"\"\"\n",
    "    def __init__(self, output_nc=3, ngf=64, z_nc=128, img_f=1024, L=1, layers=6, norm='batch', activation='ReLU',\n",
    "                 output_scale=1, use_spect=True, use_coord=False, use_attn=True):\n",
    "        super(ResGenerator, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.L = L\n",
    "        self.output_scale = output_scale\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # latent z to feature\n",
    "        mult = min(2 ** (layers-1), img_f // ngf)\n",
    "        # input -> hidden\n",
    "        self.generator = ResBlock(z_nc, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "        # transform\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'generator' + str(i), block)\n",
    "\n",
    "        # decoder part\n",
    "        for i in range(layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (layers - i - 1), img_f // ngf)\n",
    "            if i > layers - output_scale:\n",
    "                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            else:\n",
    "                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev , ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            setattr(self, 'decoder' + str(i), upconv)\n",
    "            # output part\n",
    "            if i > layers - output_scale - 1:\n",
    "                outconv = Output(ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord)\n",
    "                setattr(self, 'out' + str(i), outconv)\n",
    "            # short+long term attention part\n",
    "            if i == 1 and use_attn:\n",
    "                attn = Auto_Attn(ngf*mult, None)\n",
    "                setattr(self, 'attn' + str(i), attn)\n",
    "\n",
    "    def forward(self, z, f_m=None, f_e=None, mask=None):\n",
    "        \"\"\"\n",
    "        ResNet Generator Network\n",
    "        :param z: latent vector\n",
    "        :param f_m: feature of valid regions for conditional VAG-GAN\n",
    "        :param f_e: previous encoder feature for short+long term attention layer\n",
    "        :return results: different scale generation outputs\n",
    "        \"\"\"\n",
    "\n",
    "        f = self.generator(z)\n",
    "        for i in range(self.L):\n",
    "             generator = getattr(self, 'generator' + str(i)) # dimension not change\n",
    "             f = generator(f)\n",
    "\n",
    "        # the features come from mask regions and valid regions, we directly add them together\n",
    "        out = f_m + f\n",
    "        results= []\n",
    "        attn = 0\n",
    "        for i in range(self.layers):\n",
    "            model = getattr(self, 'decoder' + str(i))\n",
    "            out = model(out)\n",
    "            if i == 1 and self.use_attn:\n",
    "                # auto attention\n",
    "                model = getattr(self, 'attn' + str(i))\n",
    "                out, attn = model(out, f_e, mask)\n",
    "            if i > self.layers - self.output_scale - 1:\n",
    "                model = getattr(self, 'out' + str(i))\n",
    "                output = model(out)\n",
    "                results.append(output)\n",
    "                out = torch.cat([out, output], dim=1)\n",
    "\n",
    "        return results, attn\n",
    "\n",
    "class ContrastResGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrast ResNet Generator Network\n",
    "    :param output_nc: number of channels in output\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param output_scale: Different output scales\n",
    "    \"\"\"\n",
    "    def __init__(self, output_nc=3, ngf=64, z_nc=128, img_f=1024, L=1, layers=6, norm='batch', activation='ReLU',\n",
    "                 output_scale=1, use_spect=True, use_coord=False, use_attn=True):\n",
    "        super(ContrastResGenerator, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.L = L\n",
    "        self.output_scale = output_scale\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # latent z to feature\n",
    "        mult = min(2 ** (layers-1), img_f // ngf)\n",
    "        # input -> hidden\n",
    "        self.generator = ResBlock(z_nc, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "        # transform\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'generator' + str(i), block)\n",
    "\n",
    "        # decoder part\n",
    "        for i in range(layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (layers - i - 1), img_f // ngf)\n",
    "            if i > layers - output_scale:\n",
    "                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            else:\n",
    "                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev , ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            setattr(self, 'decoder' + str(i), upconv)\n",
    "            # output part\n",
    "            if i > layers - output_scale - 1:\n",
    "                outconv = Output(ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord)\n",
    "                setattr(self, 'out' + str(i), outconv)\n",
    "            # short+long term attention part\n",
    "            if i == 1 and use_attn:\n",
    "                attn = Auto_Attn(ngf*mult, None)\n",
    "                setattr(self, 'attn' + str(i), attn)\n",
    "\n",
    "    def forward(self, z, f_m=None, f_e=None, mask=None):\n",
    "        \"\"\"\n",
    "        ResNet Generator Network\n",
    "        :param z: latent vector\n",
    "        :param f_m: feature of valid regions for conditional VAG-GAN\n",
    "        :param f_e: previous encoder feature for short+long term attention layer\n",
    "        :return results: different scale generation outputs\n",
    "        \"\"\"\n",
    "\n",
    "        f = self.generator(z)\n",
    "        for i in range(self.L):\n",
    "             generator = getattr(self, 'generator' + str(i)) # dimension not change\n",
    "             f = generator(f)\n",
    "\n",
    "        # the features come from mask regions and valid regions, we directly add them together\n",
    "        # out = f_m + f\n",
    "        out = f\n",
    "        results= []\n",
    "        attn = 0\n",
    "        for i in range(self.layers):\n",
    "            model = getattr(self, 'decoder' + str(i))\n",
    "            out = model(out)\n",
    "            if i == 1 and self.use_attn:\n",
    "                # auto attention\n",
    "                model = getattr(self, 'attn' + str(i))\n",
    "                out, attn = model(out, f_e, mask)\n",
    "            if i > self.layers - self.output_scale - 1:\n",
    "                model = getattr(self, 'out' + str(i))\n",
    "                output = model(out)\n",
    "                results.append(output)\n",
    "                out = torch.cat([out, output], dim=1)\n",
    "\n",
    "        return results, attn\n",
    "\n",
    "class HiddenResGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Generator Network\n",
    "    :param output_nc: number of channels in output\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param output_scale: Different output scales\n",
    "    \"\"\"\n",
    "    def __init__(self, output_nc=3, f_text_dim=384, ngf=64, z_nc=128, img_f=256, L=1, layers=6, norm='batch', activation='ReLU',\n",
    "                 output_scale=1, use_spect=True, use_coord=False, use_attn=True):\n",
    "        super(HiddenResGenerator, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.L = L\n",
    "        self.output_scale = output_scale\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # latent z to feature\n",
    "        mult = min(2 ** (layers-1), img_f // ngf)\n",
    "        # input -> hidden\n",
    "        self.generator = ResBlock(z_nc, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.f_transformer = ResBlock(f_text_dim, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "        # transform\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'generator' + str(i), block)\n",
    "\n",
    "        # decoder part\n",
    "        for i in range(layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (layers - i - 1), img_f // ngf)\n",
    "            if i > layers - output_scale:\n",
    "                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            else:\n",
    "                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev , ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            setattr(self, 'decoder' + str(i), upconv)\n",
    "            # output part\n",
    "            if i > layers - output_scale - 1:\n",
    "                outconv = Output(ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord)\n",
    "                setattr(self, 'out' + str(i), outconv)\n",
    "            # short+long term attention part\n",
    "            if i == 1 and use_attn:\n",
    "                attn = Auto_Attn(ngf*mult, None)\n",
    "                setattr(self, 'attn' + str(i), attn)\n",
    "\n",
    "    def forward(self, z, f_text=None, f_e=None, mask=None):\n",
    "        \"\"\"\n",
    "        ResNet Generator Network\n",
    "        :param z: latent vector\n",
    "        :param f_m: feature of valid regions for conditional VAG-GAN\n",
    "        :param f_e: previous encoder feature for short+long term attention layer\n",
    "        :return results: different scale generation outputs\n",
    "        \"\"\"\n",
    "        f = self.generator(z)\n",
    "        for i in range(self.L):\n",
    "             generator = getattr(self, 'generator' + str(i)) # dimension not change\n",
    "             f = generator(f)\n",
    "        f_text_trans = self.f_transformer(f_text)\n",
    "        # the features come from mask regions and valid regions, we directly add them together\n",
    "        out = f_text_trans + f\n",
    "        results= []\n",
    "        attn = 0\n",
    "        for i in range(self.layers):\n",
    "            model = getattr(self, 'decoder' + str(i))\n",
    "            out = model(out)\n",
    "            if i == 1 and self.use_attn:\n",
    "                # auto attention\n",
    "                model = getattr(self, 'attn' + str(i))\n",
    "                out, attn = model(out, f_e, mask)\n",
    "            if i > self.layers - self.output_scale - 1:\n",
    "                model = getattr(self, 'out' + str(i))\n",
    "                output = model(out)\n",
    "                results.append(output)\n",
    "                out = torch.cat([out, output], dim=1)\n",
    "\n",
    "        return results, attn\n",
    "\n",
    "class TextualResGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Textual ResNet Generator Network.\n",
    "    This fucking code is hard to maintenance, just list a trip of shit.\n",
    "    :param output_nc: number of channels in output\n",
    "    :param ngf: base filter channel\n",
    "    :param z_nc: latent channels\n",
    "    :param img_f: the largest feature channels\n",
    "    :param L: Number of refinements of density\n",
    "    :param layers: down and up sample layers\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param output_scale: Different output scales\n",
    "    \"\"\"\n",
    "    def __init__(self, output_nc=3, f_text_dim=384, ngf=64, z_nc=128, img_f=256, L=1, layers=6, norm='batch', activation='ReLU',\n",
    "                 output_scale=1, use_spect=True, use_coord=False, use_attn=True):\n",
    "        super(TextualResGenerator, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.L = L\n",
    "        self.output_scale = output_scale\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        # latent z to feature\n",
    "        mult = min(2 ** (layers-1), img_f // ngf)\n",
    "        # input -> hidden\n",
    "        self.generator = ResBlock(z_nc, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.f_transformer = ResBlock(f_text_dim, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "\n",
    "        # transform\n",
    "        for i in range(self.L):\n",
    "            block = ResBlock(ngf * mult, ngf * mult, ngf * mult, None, nonlinearity, 'none', use_spect, use_coord)\n",
    "            setattr(self, 'generator' + str(i), block)\n",
    "\n",
    "        # decoder part\n",
    "        for i in range(layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (layers - i - 1), img_f // ngf)\n",
    "            if i > layers - output_scale:\n",
    "                # upconv = ResBlock(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev + output_nc, ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            else:\n",
    "                # upconv = ResBlock(ngf * mult_prev, ngf * mult, ngf * mult, norm_layer, nonlinearity, 'up', True)\n",
    "                upconv = ResBlockDecoder(ngf * mult_prev , ngf * mult, ngf * mult, norm_layer, nonlinearity, use_spect, use_coord)\n",
    "            setattr(self, 'decoder' + str(i), upconv)\n",
    "            # output part\n",
    "            if i > layers - output_scale - 1:\n",
    "                outconv = Output(ngf * mult, output_nc, 3, None, nonlinearity, use_spect, use_coord)\n",
    "                setattr(self, 'out' + str(i), outconv)\n",
    "            # short+long term attention part\n",
    "            if i == 1 and use_attn:\n",
    "                attn = Auto_Attn(ngf*mult, None)\n",
    "                setattr(self, 'attn' + str(i), attn)\n",
    "\n",
    "    def forward(self, z, f_text=None, f_e=None, mask=None):\n",
    "        \"\"\"\n",
    "        ResNet Generator Network\n",
    "        :param z: latent vector\n",
    "        :param f_m: feature of valid regions for conditional VAG-GAN\n",
    "        :param f_e: previous encoder feature for short+long term attention layer\n",
    "        :return results: different scale generation outputs\n",
    "        \"\"\"\n",
    "        f = self.generator(z)\n",
    "        for i in range(self.L):\n",
    "             generator = getattr(self, 'generator' + str(i)) # dimension not change\n",
    "             f = generator(f)\n",
    "        f_text_trans = self.f_transformer(f_text)\n",
    "        # the features come from mask regions and valid regions, we directly add them together\n",
    "        out = f_text_trans + f\n",
    "        results= []\n",
    "        attn = 0\n",
    "        for i in range(self.layers):\n",
    "            model = getattr(self, 'decoder' + str(i))\n",
    "            out = model(out)\n",
    "            if i == 1 and self.use_attn:\n",
    "                # auto attention\n",
    "                model = getattr(self, 'attn' + str(i))\n",
    "                out, attn = model(out, f_e, mask)\n",
    "            if i > self.layers - self.output_scale - 1:\n",
    "                model = getattr(self, 'out' + str(i))\n",
    "                output = model(out)\n",
    "                results.append(output)\n",
    "                out = torch.cat([out, output], dim=1)\n",
    "\n",
    "        return results, attn\n",
    "\n",
    "class ResDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Discriminator Network\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ndf: base filter channel\n",
    "    :param layers: down and up sample layers\n",
    "    :param img_f: the largest feature channels\n",
    "    :param norm: normalization function 'instance, batch, group'\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, ndf=64, img_f=1024, layers=6, norm='none', activation='LeakyReLU', use_spect=True,\n",
    "                 use_coord=False, use_attn=True):\n",
    "        super(ResDiscriminator, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.use_attn = use_attn\n",
    "\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        # encoder part\n",
    "        self.block0 = ResBlockEncoderOptimized(input_nc, ndf,norm_layer, nonlinearity, use_spect, use_coord)\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(layers - 1):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 1), img_f // ndf)\n",
    "            # self-attention\n",
    "            if i == 2 and use_attn:\n",
    "                attn = Auto_Attn(ndf * mult_prev, norm_layer)\n",
    "                setattr(self, 'attn' + str(i), attn)\n",
    "            block = ResBlock(ndf * mult_prev, ndf * mult, ndf * mult_prev, norm_layer, nonlinearity, 'down', use_spect, use_coord)\n",
    "            setattr(self, 'encoder' + str(i), block)\n",
    "\n",
    "        self.block1 = ResBlock(ndf * mult, ndf * mult, ndf * mult, norm_layer, nonlinearity, 'none', use_spect, use_coord)\n",
    "        self.conv = SpectralNorm(nn.Conv2d(ndf * mult, 1, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block0(x)\n",
    "        for i in range(self.layers - 1):\n",
    "            if i == 2 and self.use_attn:\n",
    "                attn = getattr(self, 'attn' + str(i))\n",
    "                out, attention = attn(out)\n",
    "            model = getattr(self, 'encoder' + str(i))\n",
    "            out = model(out)\n",
    "        out = self.block1(out)\n",
    "        out = self.conv(self.nonlinearity(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class SNPatchDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    SN Patch Discriminator Network for Local 70*70 fake/real\n",
    "    :param input_nc: number of channels in input\n",
    "    :param ndf: base filter channel\n",
    "    :param img_f: the largest channel for the model\n",
    "    :param layers: down sample layers\n",
    "    :param activation: activation function 'ReLU, SELU, LeakyReLU, PReLU'\n",
    "    :param use_spect: use spectral normalization or not\n",
    "    :param use_coord: use CoordConv or nor\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=4, ndf=64, img_f=256, layers=6, activation='LeakyReLU',\n",
    "                 use_spect=True, use_coord=False):\n",
    "        super(SNPatchDiscriminator, self).__init__()\n",
    "\n",
    "        nonlinearity = get_nonlinearity_layer(activation_type=activation)\n",
    "\n",
    "        kwargs = {'kernel_size': 4, 'stride': 2, 'padding': 1, 'bias': False}\n",
    "        sequence = [\n",
    "            coord_conv(input_nc, ndf, use_spect, use_coord, **kwargs),\n",
    "            nonlinearity,\n",
    "        ]\n",
    "\n",
    "        mult = 1\n",
    "        for i in range(1, layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** i, img_f // ndf)\n",
    "            sequence +=[\n",
    "                    coord_conv(ndf * mult_prev, ndf * mult, use_spect, use_coord, **kwargs),\n",
    "                    nonlinearity,\n",
    "                ]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ############## Text2Image Encoder-Decoder #######\n",
    "class RNN_ENCODER(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
    "                 nhidden=128, nlayers=1, bidirectional=True):\n",
    "        super(RNN_ENCODER, self).__init__()\n",
    "\n",
    "        self.n_steps = 25\n",
    "        self.rnn_type = 'LSTM'\n",
    "\n",
    "        self.ntoken = ntoken  # size of the dictionary\n",
    "        self.ninput = ninput  # size of each embedding vector\n",
    "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
    "        self.nlayers = nlayers  # Number of recurrent layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        # number of features in the hidden state\n",
    "        self.nhidden = nhidden // self.num_directions\n",
    "\n",
    "        self.define_module()\n",
    "        self.init_weights()\n",
    "\n",
    "    def define_module(self):\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
    "        self.drop = nn.Dropout(self.drop_prob)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            # dropout: If non-zero, introduces a dropout layer on\n",
    "            # the outputs of each RNN layer except the last layer\n",
    "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                               self.nlayers, batch_first=True,\n",
    "                               dropout=self.drop_prob,\n",
    "                               bidirectional=self.bidirectional)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
    "                              self.nlayers, batch_first=True,\n",
    "                              dropout=self.drop_prob,\n",
    "                              bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # Do not need to initialize RNN parameters, which have been initialized\n",
    "        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n",
    "        # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.decoder.bias.data.fill_(0)\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers * self.num_directions,\n",
    "                                        bsz, self.nhidden).zero_()),\n",
    "                    Variable(weight.new(self.nlayers * self.num_directions,\n",
    "                                        bsz, self.nhidden).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers * self.num_directions,\n",
    "                                       bsz, self.nhidden).zero_())\n",
    "\n",
    "    def forward(self, captions, cap_lens, hidden, mask=None):\n",
    "        # input: torch.LongTensor of size batch x n_steps\n",
    "        # --> emb: batch x n_steps x ninput\n",
    "        emb = self.drop(self.encoder(captions))\n",
    "        #\n",
    "        # Returns: a PackedSequence object\n",
    "        cap_lens = cap_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True, enforce_sorted=False)\n",
    "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
    "        # tensor containing the initial hidden state for each element in batch.\n",
    "        # #output (batch, seq_len, hidden_size * num_directions)\n",
    "        # #or a PackedSequence object:\n",
    "        # tensor containing output features (h_t) from the last layer of RNN\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        # PackedSequence object\n",
    "        # --> (batch, seq_len, hidden_size * num_directions)\n",
    "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
    "        # output = self.drop(output)\n",
    "        # --> batch x hidden_size*num_directions x seq_len\n",
    "        words_emb = output.transpose(1, 2)\n",
    "        # --> batch x num_directions*hidden_size\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
    "        else:\n",
    "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
    "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
    "        return words_emb, sent_emb\n",
    "\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "class CNN_ENCODER(nn.Module):\n",
    "    def __init__(self, nef, pre_train=False):\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        self.nef = nef  # define a uniform ranker\n",
    "\n",
    "        model = models.inception_v3()\n",
    "        if pre_train:\n",
    "            url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "            model.load_state_dict(model_zoo.load_url(url))\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print('Load pretrained model from ', url)\n",
    "\n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
    "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
    "        self.Mixed_5b = model.Mixed_5b\n",
    "        self.Mixed_5c = model.Mixed_5c\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        self.Mixed_6c = model.Mixed_6c\n",
    "        self.Mixed_6d = model.Mixed_6d\n",
    "        self.Mixed_6e = model.Mixed_6e\n",
    "        self.Mixed_7a = model.Mixed_7a\n",
    "        self.Mixed_7b = model.Mixed_7b\n",
    "        self.Mixed_7c = model.Mixed_7c\n",
    "\n",
    "        self.emb_features = conv1x1(768, self.nef)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # image region features\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "\n",
    "        # global image features\n",
    "        cnn_code = self.emb_cnn_code(x)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)\n",
    "        return features, cnn_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbee3aab-e0aa-4c8c-86db-54b75e337024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAnet(BaseModel):\n",
    "    \"\"\"This class implements the text-guided image completion, for 256*256 resolution\"\"\"\n",
    "    def name(self):\n",
    "        return \"TDAnet Image Completion\"\n",
    "\n",
    "    @staticmethod\n",
    "    def modify_options(parser, is_train=True):\n",
    "        \"\"\"Add new options and rewrite default values for existing options\"\"\"\n",
    "        parser.add_argument('--prior_alpha', type=float, default=0.8,\n",
    "                            help='factor to contorl prior variation: 1/(1+e^((x-0.8)*8))')\n",
    "        parser.add_argument('--prior_beta', type=float, default=8,\n",
    "                            help='factor to contorl prior variation: 1/(1+e^((x-0.8)*8))')\n",
    "        parser.add_argument('--no_maxpooling', action='store_true', help='rm maxpooling in DMA for ablation')\n",
    "        parser.add_argument('--update_language', action='store_true', help='update language encoder while training')\n",
    "        parser.add_argument('--detach_embedding', action='store_true',\n",
    "                            help='do not pass grad to embedding in DAMSM-text end')\n",
    "\n",
    "        if is_train:\n",
    "            parser.add_argument('--train_paths', type=str, default='two', help='training strategies with one path or two paths')\n",
    "            parser.add_argument('--dynamic_sigma', action='store_true', help='change sigma base on mask area')\n",
    "            parser.add_argument('--lambda_rec_l1', type=float, default=20.0, help='weight for image reconstruction loss')\n",
    "            parser.add_argument('--lambda_gen_l1', type=float, default=20.0, help='weight for image reconstruction loss')\n",
    "            parser.add_argument('--lambda_kl', type=float, default=20.0, help='weight for kl divergence loss')\n",
    "            parser.add_argument('--lambda_gan', type=float, default=1.0, help='weight for generation loss')\n",
    "            parser.add_argument('--lambda_match', type=float, default=0.1, help='weight for image-text match loss')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initial the pluralistic model\"\"\"\n",
    "        BaseModel.__init__(self, opt)\n",
    "\n",
    "        self.loss_names = ['kl_rec', 'kl_g', 'l1_rec', 'l1_g', 'gan_g', 'word_g', 'sentence_g', 'ad_l2_g',\n",
    "                           'gan_rec', 'ad_l2_rec', 'word_rec', 'sentence_rec',  'dis_img', 'dis_img_rec']\n",
    "        self.log_names = []\n",
    "        self.visual_names = ['img_m', 'img_truth', 'img_c', 'img_out', 'img_g', 'img_rec']\n",
    "        self.text_names = ['text_positive']\n",
    "        self.value_names = ['u_m', 'sigma_m', 'u_post', 'sigma_post', 'u_prior', 'sigma_prior']\n",
    "        self.model_names = ['E', 'G', 'D', 'D_rec']\n",
    "        self.distribution = []\n",
    "        self.prior_alpha = opt.prior_alpha\n",
    "        self.prior_beta = opt.prior_beta\n",
    "        self.max_pool = None if opt.no_maxpooling else 'max'\n",
    "\n",
    "        # define the inpainting model\n",
    "        self.net_E = network.define_att_textual_e(ngf=32, z_nc=256, img_f=256, layers=5, norm='none', activation='LeakyReLU',\n",
    "                          init_type='orthogonal', gpu_ids=opt.gpu_ids, image_dim=256, text_dim=256, multi_peak=False, pool_attention=self.max_pool)\n",
    "        self.net_G = network.define_hidden_textual_g(f_text_dim=768, ngf=32, z_nc=256, img_f=256, L=0, layers=5, output_scale=opt.output_scale,\n",
    "                                      norm='instance', activation='LeakyReLU', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
    "        # define the discriminator model\n",
    "        self.net_D = network.define_d(ndf=32, img_f=128, layers=5, model_type='ResDis', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
    "        self.net_D_rec = network.define_d(ndf=32, img_f=128, layers=5, model_type='ResDis', init_type='orthogonal', gpu_ids=opt.gpu_ids)\n",
    "\n",
    "        text_config = TextConfig(opt.text_config)\n",
    "        self._init_language_model(text_config)\n",
    "\n",
    "        if self.isTrain:\n",
    "            # define the loss functions\n",
    "            self.GANloss = external_function.GANLoss(opt.gan_mode)\n",
    "            self.L1loss = torch.nn.L1Loss()\n",
    "            self.L2loss = torch.nn.MSELoss()\n",
    "\n",
    "            self.image_encoder = network.CNN_ENCODER(text_config.EMBEDDING_DIM)\n",
    "            state_dict = torch.load(\n",
    "                text_config.IMAGE_ENCODER, map_location=lambda storage, loc: storage)\n",
    "            self.image_encoder.load_state_dict(state_dict)\n",
    "            self.image_encoder.eval()\n",
    "            if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "                self.image_encoder.cuda()\n",
    "            base_function._freeze(self.image_encoder)\n",
    "\n",
    "            # define the optimizer\n",
    "            self.optimizer_G = torch.optim.Adam(itertools.chain(filter(lambda p: p.requires_grad, self.net_G.parameters()),\n",
    "                        filter(lambda p: p.requires_grad, self.net_E.parameters())), lr=opt.lr, betas=(0.0, 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(itertools.chain(filter(lambda p: p.requires_grad, self.net_D.parameters()),\n",
    "                                                filter(lambda p: p.requires_grad, self.net_D_rec.parameters())),\n",
    "                                                lr=opt.lr, betas=(0.0, 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "        self.setup(opt)\n",
    "\n",
    "    def _init_language_model(self, text_config):\n",
    "        x = pickle.load(open(text_config.VOCAB, 'rb'))\n",
    "        self.ixtoword = x[2]\n",
    "        self.wordtoix = x[3]\n",
    "\n",
    "        word_len = len(self.wordtoix)\n",
    "        self.text_encoder = network.RNN_ENCODER(word_len, nhidden=256)\n",
    "\n",
    "        state_dict = torch.load(text_config.LANGUAGE_ENCODER, map_location=lambda storage, loc: storage)\n",
    "        self.text_encoder.load_state_dict(state_dict)\n",
    "        self.text_encoder.eval()\n",
    "        if not self.opt.update_language:\n",
    "            self.text_encoder.requires_grad_(False)\n",
    "        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n",
    "            self.text_encoder.cuda()\n",
    "\n",
    "    def set_input(self, input, epoch=0):\n",
    "        \"\"\"Unpack input data from the data loader and perform necessary pre-process steps\"\"\"\n",
    "        self.input = input\n",
    "        self.image_paths = self.input['img_path']\n",
    "        self.img = input['img']\n",
    "        self.mask = input['mask']\n",
    "        self.caption_idx = input['caption_idx']\n",
    "        self.caption_length = input['caption_len']\n",
    "\n",
    "        if len(self.gpu_ids) > 0:\n",
    "            self.img = self.img.cuda(self.gpu_ids[0], True)\n",
    "            self.mask = self.mask.cuda(self.gpu_ids[0], True)\n",
    "\n",
    "        # get I_m and I_c for image with mask and complement regions for training\n",
    "        self.img_truth = self.img * 2 - 1\n",
    "        self.img_m = self.mask * self.img_truth\n",
    "        self.img_c =  (1 - self.mask) * self.img_truth\n",
    "\n",
    "        # get multiple scales image ground truth and mask for training\n",
    "        self.scale_img = task.scale_pyramid(self.img_truth, self.opt.output_scale)\n",
    "        self.scale_mask = task.scale_pyramid(self.mask, self.opt.output_scale)\n",
    "\n",
    "        # About text stuff\n",
    "        self.text_positive = util.idx_to_caption(\n",
    "                                    self.ixtoword, self.caption_idx[0].tolist(), self.caption_length[0].item())\n",
    "        self.word_embeddings, self.sentence_embedding = util.vectorize_captions_idx_batch(\n",
    "                                                    self.caption_idx, self.caption_length, self.text_encoder)\n",
    "        self.text_mask = util.lengths_to_mask(self.caption_length, max_length=self.word_embeddings.size(-1))\n",
    "        self.match_labels = torch.LongTensor(range(len(self.img_m)))\n",
    "        if len(self.gpu_ids) > 0:\n",
    "            self.word_embeddings = self.word_embeddings.cuda(self.gpu_ids[0], True)\n",
    "            self.sentence_embedding = self.sentence_embedding.cuda(self.gpu_ids[0], True)\n",
    "            self.text_mask = self.text_mask.cuda(self.gpu_ids[0], True)\n",
    "            self.match_labels = self.match_labels.cuda(self.gpu_ids[0], True)\n",
    "\n",
    "    def test(self, mark=None):\n",
    "        \"\"\"Forward function used in test time\"\"\"\n",
    "        # save the groundtruth and masked image\n",
    "        self.save_results(self.img_truth, data_name='truth')\n",
    "        self.save_results(self.img_m, data_name='mask')\n",
    "\n",
    "        # encoder process\n",
    "        distribution, f, f_text = self.net_E(\n",
    "            self.img_m, self.sentence_embedding, self.word_embeddings, self.text_mask, self.mask)\n",
    "        variation_factor = 0. if self.opt.no_variance else 1.\n",
    "        q_distribution = torch.distributions.Normal(distribution[-1][0], distribution[-1][1] * variation_factor)\n",
    "        scale_mask = task.scale_img(self.mask, size=[f[2].size(2), f[2].size(3)])\n",
    "\n",
    "        # decoder process\n",
    "        for i in range(self.opt.nsampling):\n",
    "            z = q_distribution.sample()\n",
    "\n",
    "            self.img_g, attn = self.net_G(z, f_text, f_e=f[2], mask=scale_mask.chunk(3, dim=1)[0])\n",
    "            self.img_out = (1 - self.mask) * self.img_g[-1].detach() + self.mask * self.img_m\n",
    "            self.score = self.net_D(self.img_out)\n",
    "            self.save_results(self.img_out, i, data_name='out', mark=mark)\n",
    "\n",
    "    def get_distribution(self, distribution_factors):\n",
    "        \"\"\"Calculate encoder distribution for img_m, img_c only in train, all about distribution layer of VAE model\"\"\"\n",
    "        # get distribution\n",
    "        sum_valid = (torch.mean(self.mask.view(self.mask.size(0), -1), dim=1) - 1e-5).view(-1, 1, 1, 1)\n",
    "        m_sigma = 1 if not self.opt.dynamic_sigma else (1 / (1 + ((sum_valid - self.prior_alpha) * self.prior_beta).exp_()))\n",
    "        p_distribution, q_distribution, kl_rec, kl_g = 0, 0, 0, 0\n",
    "        self.distribution = []\n",
    "        for distribution in distribution_factors:\n",
    "            p_mu, p_sigma, q_mu, q_sigma = distribution\n",
    "            # the assumption distribution for different mask regions\n",
    "            std_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma))\n",
    "            # m_distribution = torch.distributions.Normal(torch.zeros_like(p_mu), torch.ones_like(p_sigma))\n",
    "            # the post distribution from mask regions\n",
    "            p_distribution = torch.distributions.Normal(p_mu, p_sigma)\n",
    "            p_distribution_fix = torch.distributions.Normal(p_mu.detach(), p_sigma.detach())\n",
    "            # the prior distribution from valid region\n",
    "            q_distribution = torch.distributions.Normal(q_mu, q_sigma)\n",
    "\n",
    "            # kl divergence\n",
    "            kl_rec += torch.distributions.kl_divergence(std_distribution, p_distribution)\n",
    "            if self.opt.train_paths == \"one\":\n",
    "                kl_g += torch.distributions.kl_divergence(std_distribution, q_distribution)\n",
    "            elif self.opt.train_paths == \"two\":\n",
    "                kl_g += torch.distributions.kl_divergence(p_distribution_fix, q_distribution)\n",
    "            self.distribution.append([torch.zeros_like(p_mu), m_sigma * torch.ones_like(p_sigma), p_mu, p_sigma, q_mu, q_sigma])\n",
    "\n",
    "        return p_distribution, q_distribution, kl_rec, kl_g\n",
    "\n",
    "    def get_G_inputs(self, p_distribution, q_distribution, f):\n",
    "        \"\"\"Process the encoder feature and distributions for generation network, combine two dataflow when implement.\"\"\"\n",
    "        f_m = torch.cat([f[-1].chunk(2)[0], f[-1].chunk(2)[0]], dim=0)\n",
    "        f_e = torch.cat([f[2].chunk(2)[0], f[2].chunk(2)[0]], dim=0)\n",
    "        scale_mask = task.scale_img(self.mask, size=[f_e.size(2), f_e.size(3)])\n",
    "        mask = torch.cat([scale_mask.chunk(3, dim=1)[0], scale_mask.chunk(3, dim=1)[0]], dim=0)\n",
    "        z_p = p_distribution.rsample()\n",
    "        z_q = q_distribution.rsample()\n",
    "        z = torch.cat([z_p, z_q], dim=0)\n",
    "        return z, f_m, f_e, mask\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Run forward processing to get the inputs\"\"\"\n",
    "        # encoder process\n",
    "        distribution_factors, f, f_text = self.net_E(\n",
    "            self.img_m, self.sentence_embedding, self.word_embeddings, self.text_mask, self.mask, self.img_c)\n",
    "\n",
    "        p_distribution, q_distribution, self.kl_rec, self.kl_g = self.get_distribution(distribution_factors)\n",
    "\n",
    "        # decoder process\n",
    "        z, f_m, f_e, mask = self.get_G_inputs(p_distribution, q_distribution, f) # prepare inputs: img, mask, distribute\n",
    "\n",
    "        results, attn = self.net_G(z, f_text, f_e, mask)\n",
    "        self.img_rec = []\n",
    "        self.img_g = []\n",
    "        for result in results:\n",
    "            img_rec, img_g = result.chunk(2)\n",
    "            self.img_rec.append(img_rec)\n",
    "            self.img_g.append(img_g)\n",
    "        self.img_out = (1-self.mask) * self.img_g[-1].detach() + self.mask * self.img_truth\n",
    "\n",
    "        self.region_features_rec, self.cnn_code_rec = self.image_encoder(self.img_rec[-1])\n",
    "        self.region_features_g, self.cnn_code_g = self.image_encoder(self.img_g[-1])\n",
    "\n",
    "\n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
    "        # Real\n",
    "        D_real = netD(real)\n",
    "        D_real_loss = self.GANloss(D_real, True, True)\n",
    "        # fake\n",
    "        D_fake = netD(fake.detach())\n",
    "        D_fake_loss = self.GANloss(D_fake, False, True)\n",
    "        # loss for discriminator\n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        # gradient penalty for wgan-gp\n",
    "        if self.opt.gan_mode == 'wgangp':\n",
    "            gradient_penalty, gradients = external_function.cal_gradient_penalty(netD, real, fake.detach())\n",
    "            D_loss +=gradient_penalty\n",
    "\n",
    "        D_loss.backward()\n",
    "\n",
    "        return D_loss\n",
    "\n",
    "    def backward_D(self):\n",
    "        \"\"\"Calculate the GAN loss for the discriminators\"\"\"\n",
    "        base_function._unfreeze(self.net_D, self.net_D_rec)\n",
    "        ## Note: changed gen path gan loss to rec path\n",
    "        # self.loss_dis_img = self.backward_D_basic(self.net_D, self.img_truth, self.img_g[-1])\n",
    "        self.loss_dis_img = self.backward_D_basic(self.net_D, self.img_truth, self.img_rec[-1])\n",
    "        self.loss_dis_img_rec = self.backward_D_basic(self.net_D_rec, self.img_truth, self.img_rec[-1])\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"Calculate training loss for the generator\"\"\"\n",
    "\n",
    "        # encoder kl loss\n",
    "        self.loss_kl_rec = self.kl_rec.mean() * self.opt.lambda_kl * self.opt.output_scale\n",
    "        self.loss_kl_g = self.kl_g.mean() * self.opt.lambda_kl * self.opt.output_scale\n",
    "\n",
    "        # Adversarial loss\n",
    "        base_function._freeze(self.net_D, self.net_D_rec)\n",
    "\n",
    "        # D loss fake\n",
    "        D_fake_g = self.net_D(self.img_g[-1])\n",
    "        self.loss_gan_g = self.GANloss(D_fake_g, True, False) * self.opt.lambda_gan\n",
    "        D_fake_rec = self.net_D(self.img_rec[-1])\n",
    "        self.loss_gan_rec = self.GANloss(D_fake_rec, True, False) * self.opt.lambda_gan\n",
    "\n",
    "        # LSGAN loss\n",
    "        D_fake = self.net_D_rec(self.img_rec[-1])\n",
    "        D_real = self.net_D_rec(self.img_truth)\n",
    "        D_fake_g = self.net_D_rec(self.img_g[-1])\n",
    "        self.loss_ad_l2_rec = self.L2loss(D_fake, D_real) * self.opt.lambda_gan\n",
    "        self.loss_ad_l2_g = self.L2loss(D_fake_g, D_real) * self.opt.lambda_gan\n",
    "\n",
    "        # Text-image consistent loss\n",
    "        if not self.opt.detach_embedding:\n",
    "            sentence_embedding = self.sentence_embedding\n",
    "            word_embeddings = self.word_embeddings\n",
    "        else:\n",
    "            sentence_embedding = self.sentence_embedding.detach()\n",
    "            word_embeddings = self.word_embeddings.detach()\n",
    "\n",
    "\n",
    "        loss_sentence = base_function.sent_loss(self.cnn_code_rec, sentence_embedding, self.match_labels)\n",
    "        loss_word, _ = base_function.words_loss(self.region_features_rec, word_embeddings, self.match_labels, \\\n",
    "                                 self.caption_length, len(word_embeddings))\n",
    "        self.loss_word_rec = loss_word * self.opt.lambda_match\n",
    "        self.loss_sentence_rec = loss_sentence * self.opt.lambda_match\n",
    "\n",
    "        loss_sentence = base_function.sent_loss(self.cnn_code_g, sentence_embedding, self.match_labels)\n",
    "        loss_word, _ = base_function.words_loss(self.region_features_g, word_embeddings, self.match_labels, \\\n",
    "                                 self.caption_length, len(word_embeddings))\n",
    "        self.loss_word_g = loss_word * self.opt.lambda_match\n",
    "        self.loss_sentence_g = loss_sentence * self.opt.lambda_match\n",
    "\n",
    "\n",
    "        # calculate l1 loss ofr multi-scale, multi-depth-level outputs\n",
    "        loss_l1_rec, loss_l1_g, log_PSNR_rec, log_PSNR_out = 0, 0, 0, 0\n",
    "        for i, (img_rec_i, img_fake_i, img_out_i, img_real_i, mask_i) in enumerate(zip(self.img_rec, self.img_g, self.img_out, self.scale_img, self.scale_mask)):\n",
    "            loss_l1_rec += self.L1loss(img_rec_i, img_real_i)\n",
    "            if self.opt.train_paths == \"one\":\n",
    "                loss_l1_g += self.L1loss(img_fake_i, img_real_i)\n",
    "            elif self.opt.train_paths == \"two\":\n",
    "                loss_l1_g += self.L1loss(img_fake_i, img_real_i)\n",
    "\n",
    "        self.loss_l1_rec = loss_l1_rec * self.opt.lambda_rec_l1\n",
    "        self.loss_l1_g = loss_l1_g * self.opt.lambda_gen_l1\n",
    "\n",
    "        # if one path during the training, just calculate the loss for generation path\n",
    "        if self.opt.train_paths == \"one\":\n",
    "            self.loss_l1_rec = self.loss_l1_rec * 0\n",
    "            self.loss_ad_l2_rec = self.loss_ad_l2_rec * 0\n",
    "            self.loss_kl_rec = self.loss_kl_rec * 0\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for name in self.loss_names:\n",
    "            if name != 'dis_img' and name != 'dis_img_rec':\n",
    "                total_loss += getattr(self, \"loss_\" + name)\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"update network weights\"\"\"\n",
    "        # compute the image completion results\n",
    "        self.forward()\n",
    "        # optimize the discrinimator network parameters\n",
    "        self.optimizer_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.optimizer_D.step()\n",
    "        # optimize the completion network parameters\n",
    "        self.optimizer_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.optimizer_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c3770b1-e810-46e1-a28d-c37c8640a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This package contains modules related to function, network architectures, and models\"\"\"\n",
    "\n",
    "def find_model_using_name(model_name):\n",
    "    \"\"\"Import the module \"model/[model_name]_model.py\".\"\"\"\n",
    "    model_file_name = \"model.\" + model_name + \"_model\"\n",
    "    modellib = importlib.import_module(model_file_name)\n",
    "    model = None\n",
    "    for name, cls in modellib.__dict__.items():\n",
    "        if name.lower() == model_name.lower() and issubclass(cls, BaseModel):\n",
    "            model = cls\n",
    "\n",
    "    if model is None:\n",
    "        print(\"In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase.\" % (model_file_name, model_name))\n",
    "        exit(0)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_option_setter(model_name):\n",
    "    \"\"\"Return the static method <modify_commandline_options> of the model class.\"\"\"\n",
    "    model = find_model_using_name(model_name)\n",
    "    return model.modify_options\n",
    "\n",
    "\n",
    "def create_model(opt):\n",
    "    \"\"\"Create a model given the option.\"\"\"\n",
    "    model = find_model_using_name(opt.model)\n",
    "    instance = model(opt)\n",
    "    print(\"model [%s] was created\" % type(instance).__name__)\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cff1cef-4441-4462-a282-27ad435cf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(data.Dataset):\n",
    "    def __init__(self, opt, debug=False):\n",
    "        self.opt = opt\n",
    "        self.debug = debug\n",
    "        self.img_paths, self.img_size = make_dataset(opt.img_file)\n",
    "        # provides random file for training and testing\n",
    "        if opt.mask_file != 'none':\n",
    "            if not opt.mask_file.endswith('.json'):\n",
    "                self.mask_paths, self.mask_size = make_dataset(opt.mask_file)\n",
    "            else:\n",
    "                with open(opt.mask_file, 'r') as f:\n",
    "                    self.image_bbox = json.load(f)\n",
    "\n",
    "        self.transform = get_transform(opt)\n",
    "\n",
    "        ## ========Abnout text stuff===============\n",
    "        text_config = TextConfig(opt.text_config)\n",
    "        self.max_length = text_config.MAX_TEXT_LENGTH\n",
    "        if 'coco' in text_config.CAPTION.lower():\n",
    "            self.num_captions = 5\n",
    "        elif 'place' in text_config.CAPTION.lower():\n",
    "            self.num_captions = 1\n",
    "        else:\n",
    "            self.num_captions = 10\n",
    "\n",
    "        # load caption file\n",
    "        with open(text_config.CAPTION, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "\n",
    "        x = pickle.load(open(text_config.VOCAB, 'rb'))\n",
    "        self.ixtoword = x[2]\n",
    "        self.wordtoix = x[3]\n",
    "\n",
    "        self.epoch = 0 # Used for iter on captions.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        index = self.epoch*self.img_size+index\n",
    "\n",
    "        img, img_path = self.load_img(index)\n",
    "        # load mask\n",
    "        mask = self.load_mask(img, index, img_path)\n",
    "        assert sum(img.shape) == sum(mask.shape), (img.shape, mask.shape)\n",
    "        caption_idx, caption_len, caption, img_name= self._load_text_idx(index)\n",
    "        return {'img': img, 'img_path': img_path, 'mask': mask, \\\n",
    "                'caption_idx' : torch.Tensor(caption_idx).long(), 'caption_len':caption_len,\\\n",
    "                'caption_text': caption, 'image_path': img_name}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_size\n",
    "\n",
    "    def name(self):\n",
    "        return \"inpainting dataset\"\n",
    "\n",
    "    def load_img(self, index):\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        img_path = self.img_paths[index % self.img_size]\n",
    "        img_pil = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img_pil)\n",
    "        img_pil.close()\n",
    "        return img, img_path\n",
    "\n",
    "    def _load_text_idx(self, image_index):\n",
    "        img_name = self.img_paths[image_index % self.img_size]\n",
    "        caption_index_of_image = image_index // self.img_size  % self.num_captions\n",
    "        img_name = os.path.basename(img_name)\n",
    "        captions = self.captions[img_name]\n",
    "        caption = captions[caption_index_of_image] if type(captions) == list else captions\n",
    "        caption_idx, caption_len = util._caption_to_idx(self.wordtoix, caption, self.max_length)\n",
    "\n",
    "        return caption_idx, caption_len, caption, img_name\n",
    "\n",
    "    def load_mask(self, img, index, img_path):\n",
    "        \"\"\"Load different mask types for training and testing\"\"\"\n",
    "        mask_type_index = random.randint(0, len(self.opt.mask_type) - 1)\n",
    "        mask_type = self.opt.mask_type[mask_type_index]\n",
    "\n",
    "        # center mask\n",
    "        if mask_type == 0:\n",
    "            return task.center_mask(img)\n",
    "\n",
    "        # random regular mask\n",
    "        if mask_type == 1:\n",
    "            return task.random_regular_mask(img)\n",
    "\n",
    "        # random irregular mask\n",
    "        if mask_type == 2:\n",
    "            return task.random_irregular_mask(img)\n",
    "\n",
    "        if mask_type == 3:\n",
    "            # file masks, e.g. CUB object mask\n",
    "            mask_index = index\n",
    "            mask_pil = Image.open(self.mask_paths[mask_index]).convert('RGB')\n",
    "\n",
    "            mask_transform = get_transform_mask(self.opt)\n",
    "\n",
    "            mask = (mask_transform(mask_pil) == 0).float()\n",
    "            mask_pil.close()\n",
    "            return mask\n",
    "\n",
    "        if mask_type == 4:\n",
    "            # coco json file object mask\n",
    "            if os.path.basename(img_path) not in self.image_bbox:\n",
    "                return task.random_regular_mask(img)\n",
    "\n",
    "            img_original = np.asarray(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "            # create a mask matrix same as img_original\n",
    "            mask = np.zeros_like(img_original)\n",
    "            bboxes = self.image_bbox[os.path.basename(img_path)]\n",
    "\n",
    "            # choose max area box\n",
    "            choosen_box = 0,0,0,0\n",
    "            max_area = 0\n",
    "            for x1,x2,y1,y2 in bboxes:\n",
    "                area = (x2-x1) * (y2-y1)\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    choosen_box = x1,x2,y1,y2\n",
    "            x1, x2, y1, y2 = choosen_box\n",
    "            mask[x1:x2, y1:y2] = 1\n",
    "\n",
    "            # apply same transform as img to the mask\n",
    "            mask_pil = Image.fromarray(mask)\n",
    "\n",
    "            mask_transform = get_transform_mask(self.opt)\n",
    "\n",
    "            mask = (mask_transform(mask_pil) == 0).float()\n",
    "\n",
    "            mask_pil.close()\n",
    "\n",
    "            return mask\n",
    "\n",
    "\n",
    "def dataloader(opt):\n",
    "    datasets = CreateDataset(opt)\n",
    "    dataset = data.DataLoader(datasets, batch_size=opt.batchSize, shuffle=not opt.no_shuffle, num_workers=int(opt.nThreads), pin_memory=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_transform_mask(opt):\n",
    "    \"\"\"Basic process to transform PIL image to torch tensor\"\"\"\n",
    "    transform_list = []\n",
    "    osize = [opt.loadSize[0], opt.loadSize[1]]\n",
    "    fsize = [opt.fineSize[0], opt.fineSize[1]]\n",
    "    if opt.isTrain:\n",
    "        if opt.resize_or_crop == 'resize_and_crop':\n",
    "            transform_list.append(transforms.Resize(osize))\n",
    "            transform_list.append(transforms.RandomCrop(fsize))\n",
    "        elif opt.resize_or_crop == 'crop':\n",
    "            transform_list.append(transforms.RandomCrop(fsize))\n",
    "        if not opt.no_flip:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        if not opt.no_rotation:\n",
    "            transform_list.append(transforms.RandomRotation(3))\n",
    "    else:\n",
    "        transform_list.append(transforms.Resize(fsize))\n",
    "\n",
    "    transform_list += [transforms.ToTensor()]\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def get_transform(opt):\n",
    "    \"\"\"Basic process to transform PIL image to torch tensor\"\"\"\n",
    "    transform_list = []\n",
    "    osize = [opt.loadSize[0], opt.loadSize[1]]\n",
    "    fsize = [opt.fineSize[0], opt.fineSize[1]]\n",
    "    if opt.isTrain:\n",
    "        if opt.resize_or_crop == 'resize_and_crop':\n",
    "            transform_list.append(transforms.Resize(osize))\n",
    "            transform_list.append(transforms.RandomCrop(fsize))\n",
    "        elif opt.resize_or_crop == 'crop':\n",
    "            transform_list.append(transforms.RandomCrop(fsize))\n",
    "        if not opt.no_augment:\n",
    "            transform_list.append(transforms.ColorJitter(0.0, 0.0, 0.0, 0.0))\n",
    "        if not opt.no_flip:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        if not opt.no_rotation:\n",
    "            transform_list.append(transforms.RandomRotation(3))\n",
    "    else:\n",
    "        transform_list.append(transforms.Resize(fsize))\n",
    "\n",
    "    transform_list += [transforms.ToTensor()]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c80c3a2-50bd-42eb-932e-37266ade4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(path_files):\n",
    "    if os.path.isfile(path_files):\n",
    "        paths, size = make_dataset_txt(path_files)\n",
    "    else:\n",
    "        paths, size = make_dataset_dir(path_files)\n",
    "\n",
    "    return paths, size\n",
    "\n",
    "\n",
    "def make_dataset_txt(files):\n",
    "    \"\"\"\n",
    "    :param path_files: the path of txt file that store the image paths\n",
    "    :return: image paths and sizes\n",
    "    \"\"\"\n",
    "    img_paths = []\n",
    "\n",
    "    with open(files) as f:\n",
    "        paths = f.readlines()\n",
    "\n",
    "    for path in paths:\n",
    "        path = path.strip()\n",
    "        img_paths.append(os.path.join(os.path.dirname(files), path))\n",
    "\n",
    "    return img_paths, len(img_paths)\n",
    "\n",
    "\n",
    "def make_dataset_dir(dir):\n",
    "    \"\"\"\n",
    "    :param dir: directory paths that store the image\n",
    "    :return: image paths and sizes\n",
    "    \"\"\"\n",
    "    img_paths = []\n",
    "\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    for root, _, fnames in os.walk(dir):\n",
    "        for fname in sorted(fnames):\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                img_paths.append(path)\n",
    "\n",
    "    return img_paths, len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c74e720-e0f9-4462-8be1-55b9a4931273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import model\n",
    "from util import util\n",
    "\n",
    "\n",
    "class BaseOptions():\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, parser):\n",
    "        # base define\n",
    "        parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment.')\n",
    "        parser.add_argument('--model', type=str, default='tdanet', help='name of the model type. [pluralistic]')\n",
    "        parser.add_argument('--mask_type', type=int, default=[1, 2, 3], nargs='+',\n",
    "                            help='mask type, 0: center mask, 1:random regular mask, '\n",
    "                            '2: random irregular mask. 3: external irregular mask. 4: external json bbox mask'\n",
    "                            ' [0],[1,2],[1,2,3]')\n",
    "        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are save here')\n",
    "        parser.add_argument('--which_iter', type=str, default='latest', help='which iterations to load')\n",
    "        parser.add_argument('--gpu_ids', type=str, default='-1', help='gpu ids: e.g. 0, 1, 2 use -1 for CPU')\n",
    "        parser.add_argument('--text_config', type=str, default='config.bird.yml', help='path to text config')\n",
    "        parser.add_argument('--output_scale', type=int, default=4, help='# of number of the output scale')\n",
    "\n",
    "        # data pattern define\n",
    "        parser.add_argument('--img_file', type=str, default='/home/hemanthgaddey/Documents/tdanet_/tdanet/dataset/CUB_200_2011/images/002.Laysan_Albatross', help='training and testing dataset')\n",
    "        parser.add_argument('--mask_file', type=str, default='none', help='load test mask')\n",
    "        parser.add_argument('--loadSize', type=int, default=[266, 266], help='scale images to this size')\n",
    "        parser.add_argument('--fineSize', type=int, default=[256, 256], help='then crop to this size')\n",
    "        parser.add_argument('--resize_or_crop', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop|crop|]')\n",
    "        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the image for data augmentation')\n",
    "        parser.add_argument('--no_rotation', action='store_true', help='if specified, do not rotation for data augmentation')\n",
    "        parser.add_argument('--no_augment', action='store_true', help='if specified, do not augment the image for data augmentation')\n",
    "        parser.add_argument('--batchSize', type=int, default=10, help='input batch size')\n",
    "        parser.add_argument('--nThreads', type=int, default=8, help='# threads for loading data')\n",
    "        parser.add_argument('--no_shuffle', action='store_true',help='if true, takes images serial')\n",
    "\n",
    "        # display parameter define\n",
    "        parser.add_argument('--display_winsize', type=int, default=256, help='display window size')\n",
    "        parser.add_argument('--display_id', type=int, default=1, help='display id of the web')\n",
    "        parser.add_argument('--display_port', type=int, default=8097, help='visidom port of the web display')\n",
    "        parser.add_argument('--display_single_pane_ncols', type=int, default=0, help='if positive, display all images in a single visidom web panel')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def gather_options(self):\n",
    "        \"\"\"Add additional model-specific options\"\"\"\n",
    "\n",
    "        if not self.initialized:\n",
    "            parser = self.initialize(self.parser)\n",
    "\n",
    "        # get basic options\n",
    "        opt, _ = parser.parse_known_args()\n",
    "\n",
    "        # modify the options for different models\n",
    "        model_option_set = model.get_option_setter(opt.model)\n",
    "        parser = model_option_set(parser, self.isTrain)\n",
    "        opt = parser.parse_args('')\n",
    "\n",
    "        return opt\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"Parse the options\"\"\"\n",
    "\n",
    "        opt = self.gather_options()\n",
    "        opt.isTrain = self.isTrain\n",
    "\n",
    "        self.print_options(opt)\n",
    "\n",
    "        # set gpu ids\n",
    "        str_ids = opt.gpu_ids.split(',')\n",
    "        opt.gpu_ids = []\n",
    "        for str_id in str_ids:\n",
    "            id = int(str_id)\n",
    "            if id >= 0:\n",
    "                opt.gpu_ids.append(id)\n",
    "        if len(opt.gpu_ids):\n",
    "            torch.cuda.set_device(opt.gpu_ids[0])\n",
    "\n",
    "        self.opt = opt\n",
    "\n",
    "        return self.opt\n",
    "\n",
    "    @staticmethod\n",
    "    def print_options(opt):\n",
    "        \"\"\"print and save options\"\"\"\n",
    "\n",
    "        print('--------------Options--------------')\n",
    "        for k, v in sorted(vars(opt).items()):\n",
    "            print('%s: %s' % (str(k), str(v)))\n",
    "        print('----------------End----------------')\n",
    "\n",
    "        # save to the disk\n",
    "        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
    "        util.mkdirs(expr_dir)\n",
    "        if opt.isTrain:\n",
    "            file_name = os.path.join(expr_dir, 'train_opt.txt')\n",
    "        else:\n",
    "            file_name = os.path.join(expr_dir, 'test_opt.txt')\n",
    "        with open(file_name, 'wt') as opt_file:\n",
    "            opt_file.write('--------------Options--------------\\n')\n",
    "            for k, v in sorted(vars(opt).items()):\n",
    "                opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
    "            opt_file.write('----------------End----------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "164981f4-7b34-48b7-87ae-4b6f5c5d809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainOptions(BaseOptions):\n",
    "    def initialize(self, parser):\n",
    "        parser = BaseOptions.initialize(self, parser)\n",
    "\n",
    "        # training epoch\n",
    "        parser.add_argument('--iter_count', type=int, default=1, help='the starting epoch count')\n",
    "        parser.add_argument('--niter', type=int, default=5000000, help='# of iter with initial learning rate')\n",
    "        parser.add_argument('--niter_decay', type=int, default=0, help='# of iter to decay learning rate to zero')\n",
    "        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        parser.add_argument('--valid_file', type=str, default='/data/dataset/valid', help='valid dataset')\n",
    "\n",
    "        # learning rate and loss weight\n",
    "        parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy[lambda|step|plateau]')\n",
    "        parser.add_argument('--lr', type=float, default=1e-4, help='initial learning rate for adam')\n",
    "        parser.add_argument('--gan_mode', type=str, default='lsgan', choices=['wgan-gp', 'hinge', 'lsgan'])\n",
    "\n",
    "        # display the results\n",
    "        parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n",
    "        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
    "        parser.add_argument('--save_latest_freq', type=int, default=1000, help='frequency of saving the latest results at each batch')\n",
    "        parser.add_argument('--save_iters_freq', type=int, default=10000, help='frequency of saving checkpoints at the end of batch')\n",
    "        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results')\n",
    "\n",
    "        self.isTrain = True\n",
    "\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab5f224e-4ac2-4914-b47d-cf4e809e5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def idx_to_caption(ixtoword, caption, length):\n",
    "    \"\"\" Turn idx to word\"\"\"\n",
    "    return ' '.join([ixtoword[caption[i]] for i in range(length)])\n",
    "\n",
    "def _caption_to_idx(wordtoix, caption, max_length):\n",
    "    '''Transform single text caption to idx and length tensor'''\n",
    "    caption_token = word_tokenize(caption.lower())\n",
    "\n",
    "    caption_idx = []\n",
    "    for token in caption_token:\n",
    "        t = token.encode('ascii', 'ignore').decode('ascii')\n",
    "        if len(t) > 0 and t in wordtoix:\n",
    "            caption_idx.append(wordtoix[t])\n",
    "\n",
    "    length = len(caption_idx)\n",
    "    if length <= max_length:\n",
    "        caption_idx = caption_idx + [0] * (max_length - len(caption_idx))\n",
    "    else:\n",
    "        caption_idx = caption_idx[:max_length]\n",
    "\n",
    "    return caption_idx, length\n",
    "\n",
    "def vectorize_captions_idx_batch(batch_padded_captions_idx, batch_length, language_encoder):\n",
    "    '''Transform batch_padded_captions_idx to sentence embedding'''\n",
    "    batch_size = len(batch_length)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = language_encoder.init_hidden(batch_size)\n",
    "        device = hidden[0].device\n",
    "        word_embs, sent_emb = language_encoder(batch_padded_captions_idx.to(device), \\\n",
    "                                       batch_length.to(device), hidden)\n",
    "    return word_embs, sent_emb\n",
    "\n",
    "def lengths_to_mask(lengths, max_length, device=None):\n",
    "    '''transform digital lengths to tensor mask.'''\n",
    "    masks = torch.ones(len(lengths), max_length)\n",
    "    for i, length in enumerate(lengths):\n",
    "        masks[i,:length] = 0\n",
    "    masks = masks.bool()\n",
    "    return masks if device is None else masks.to(device)\n",
    "\n",
    "\n",
    "def PSNR(a, b):\n",
    "    '''compute PSNR for a and b image'''\n",
    "    mse = np.mean((a - b) ** 2) + 1e-8\n",
    "\n",
    "    return 20 * np.log10(255.0 / np.sqrt(mse))\n",
    "\n",
    "def tensor_image_scale(tensor):\n",
    "    '''scale the value in tensor as image'''\n",
    "    return (tensor + 1) / 2.0 * 255.0\n",
    "\n",
    "# convert a tensor into a numpy array\n",
    "def tensor2im(image_tensor, bytes=255.0, imtype=np.uint8):\n",
    "    if image_tensor.dim() == 3:\n",
    "        image_numpy = image_tensor.cpu().float().numpy()\n",
    "    else:\n",
    "        image_numpy = image_tensor[0].cpu().float().numpy()\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * bytes\n",
    "\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "# conver a tensor into a numpy array\n",
    "def tensor2array(value_tensor):\n",
    "    if value_tensor.dim() == 3:\n",
    "        numpy = value_tensor.view(-1).cpu().float().numpy()\n",
    "    else:\n",
    "        numpy = value_tensor[0].view(-1).cpu().float().numpy()\n",
    "    return numpy\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    if image_numpy.shape[2] == 1:\n",
    "        image_numpy = image_numpy.reshape(image_numpy.shape[0], image_numpy.shape[1])\n",
    "\n",
    "    imageio.imwrite(image_path, image_numpy)\n",
    "\n",
    "\n",
    "def mkdirs(paths):\n",
    "    if isinstance(paths, list) and not isinstance(paths, str):\n",
    "        for path in paths:\n",
    "            mkdir(path)\n",
    "    else:\n",
    "        mkdir(paths)\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "927b89a7-d8a1-404e-a03f-bc7087700853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dominate\n",
    "from dominate.tags import *\n",
    "import os\n",
    "\n",
    "\n",
    "class HTML:\n",
    "    def __init__(self, web_dir, title, reflesh=0):\n",
    "        self.title = title\n",
    "        self.web_dir = web_dir\n",
    "        self.img_dir = os.path.join(self.web_dir, 'images')\n",
    "        if not os.path.exists(self.web_dir):\n",
    "            os.makedirs(self.web_dir)\n",
    "        if not os.path.exists(self.img_dir):\n",
    "            os.makedirs(self.img_dir)\n",
    "        # print(self.img_dir)\n",
    "\n",
    "        self.doc = dominate.document(title=title)\n",
    "        if reflesh > 0:\n",
    "            with self.doc.head:\n",
    "                meta(http_equiv=\"reflesh\", content=str(reflesh))\n",
    "\n",
    "    def get_image_dir(self):\n",
    "        return self.img_dir\n",
    "\n",
    "    def add_header(self, str):\n",
    "        with self.doc:\n",
    "            h3(str)\n",
    "\n",
    "    def add_table(self, border=1):\n",
    "        self.t = table(border=border, style=\"table-layout: fixed;\")\n",
    "        self.doc.add(self.t)\n",
    "\n",
    "    def add_images(self, ims, txts, links, width=400):\n",
    "        self.add_table()\n",
    "        with self.t:\n",
    "            with tr():\n",
    "                for im, txt, link in zip(ims, txts, links):\n",
    "                    with td(style=\"word-wrap: break-word;\", halign=\"center\", valign=\"top\"):\n",
    "                        with p():\n",
    "                            with a(href=os.path.join('images', link)):\n",
    "                                img(style=\"width:%dpx\" % width, src=os.path.join('images', im))\n",
    "                            br()\n",
    "                            p(txt)\n",
    "\n",
    "    def save(self):\n",
    "        html_file = '%s/index.html' % self.web_dir\n",
    "        f = open(html_file, 'wt')\n",
    "        f.write(self.doc.render())\n",
    "        f.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    html = HTML('web/', 'test_html')\n",
    "    html.add_header('hello world')\n",
    "\n",
    "    ims = []\n",
    "    txts = []\n",
    "    links = []\n",
    "    for n in range(4):\n",
    "        ims.append('image_%d.png' % n)\n",
    "        txts.append('text_%d' % n)\n",
    "        links.append('image_%d.png' % n)\n",
    "    html.add_images(ims, txts, links)\n",
    "    html.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d26f22f8-1dfa-45da-ae9a-40fcd6f7e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ntpath\n",
    "import time\n",
    "\n",
    "class Visualizer():\n",
    "    def __init__(self, opt):\n",
    "        # self.opt = opt\n",
    "        self.display_id = opt.display_id\n",
    "        self.use_html = opt.isTrain and not opt.no_html\n",
    "        self.win_size = opt.display_winsize\n",
    "        self.name = opt.name\n",
    "        if self.display_id > 0:\n",
    "            import visdom\n",
    "            self.vis = visdom.Visdom(port = opt.display_port)\n",
    "            self.display_single_pane_ncols = opt.display_single_pane_ncols\n",
    "\n",
    "        if self.use_html:\n",
    "            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web')\n",
    "            self.img_dir = os.path.join(self.web_dir, 'images')\n",
    "            print('create web directory %s...' % self.web_dir)\n",
    "            util.mkdirs([self.web_dir, self.img_dir])\n",
    "        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.txt')\n",
    "        with open(self.log_name, \"a\") as log_file:\n",
    "            now = time.strftime(\"%c\")\n",
    "            log_file.write('================ Training Loss (%s) ================\\n' % now)\n",
    "\n",
    "    # |visuals|: dictionary of images to display or save\n",
    "    def display_current_results(self, visuals, text, epoch):\n",
    "        if self.display_id > 0: # show images in the browser\n",
    "            if self.display_single_pane_ncols > 0:\n",
    "                h, w = next(iter(visuals.values())).shape[:2]\n",
    "                table_css = \"\"\"<style>\n",
    "    table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n",
    "    table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n",
    "</style>\"\"\" % (w, h)\n",
    "                ncols = self.display_single_pane_ncols\n",
    "                title = self.name\n",
    "                label_html = ''\n",
    "                label_html_row = ''\n",
    "                nrows = int(np.ceil(len(visuals.items()) / ncols))\n",
    "                images = []\n",
    "                idx = 0\n",
    "                for label, image_numpy in visuals.items():\n",
    "                    label_html_row += '<td>%s</td>' % label\n",
    "                    images.append(image_numpy.transpose([2, 0, 1]))\n",
    "                    idx += 1\n",
    "                    if idx % ncols == 0:\n",
    "                        label_html += '<tr>%s</tr>' % label_html_row\n",
    "                        label_html_row = ''\n",
    "                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n",
    "                while idx % ncols != 0:\n",
    "                    images.append(white_image)\n",
    "                    label_html_row += '<td></td>'\n",
    "                    idx += 1\n",
    "                if label_html_row != '':\n",
    "                    label_html += '<tr>%s</tr>' % label_html_row\n",
    "                # pane col = image row\n",
    "                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n",
    "                                padding=2, opts=dict(title=title + ' images'))\n",
    "                label_html = '<table>%s</table>' % label_html\n",
    "                self.vis.text(table_css + label_html, win = self.display_id + 2,\n",
    "                              opts=dict(title=title + ' labels'))\n",
    "            else:\n",
    "                idx = 1\n",
    "                for label, image_numpy in visuals.items():\n",
    "                    #image_numpy = np.flipud(image_numpy)\n",
    "                    self.vis.image(image_numpy.transpose([2,0,1]), opts=dict(title=label),\n",
    "                                       win=self.display_id + idx)\n",
    "                    idx += 1\n",
    "\n",
    "        for key, value in text.items():\n",
    "            self.vis.text(value, win=key, opts=dict(title=key))\n",
    "\n",
    "        if self.use_html: # save images to a html file\n",
    "            for label, image_numpy in visuals.items():\n",
    "                img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))\n",
    "                util.save_image(image_numpy, img_path)\n",
    "            # update website\n",
    "            webpage = html.HTML(self.web_dir, 'Experiment name = %s' % self.name, reflesh=1)\n",
    "            for n in range(epoch, 0, -1):\n",
    "                webpage.add_header('epoch [%d]' % n)\n",
    "                ims = []\n",
    "                txts = []\n",
    "                links = []\n",
    "\n",
    "                for label, image_numpy in visuals.items():\n",
    "                    img_path = 'epoch%.3d_%s.png' % (n, label)\n",
    "                    ims.append(img_path)\n",
    "                    txts.append(label)\n",
    "                    links.append(img_path)\n",
    "                webpage.add_images(ims, txts, links, width=self.win_size)\n",
    "            webpage.save()\n",
    "\n",
    "    # errors: dictionary of error labels and values\n",
    "    def plot_current_errors(self, iters, errors):\n",
    "        if not hasattr(self, 'plot_data'):\n",
    "            self.plot_data = {'X': [], 'Y': [], 'legend': list(errors.keys())}\n",
    "        self.plot_data['X'].append(iters)\n",
    "        self.plot_data['Y'].append([errors[k] for k in self.plot_data['legend']])\n",
    "        self.vis.line(\n",
    "            X=np.stack([np.array(self.plot_data['X'])] * len(self.plot_data['legend']), 1),\n",
    "            Y=np.array(self.plot_data['Y']),\n",
    "            opts={'title': self.name + ' loss over time',\n",
    "                  'legend': self.plot_data['legend'],\n",
    "                  'xlabel': 'iterations',\n",
    "                  'ylabel': 'loss'},\n",
    "            win=self.display_id)\n",
    "\n",
    "    def plot_current_score(self, epoch, counter_ratio, scores):\n",
    "        if not hasattr(self, 'plot_score'):\n",
    "            self.plot_score = {'X':[],'Y':[], 'legend':list(scores.keys())}\n",
    "        self.plot_score['X'].append(epoch + counter_ratio)\n",
    "        self.plot_score['Y'].append([scores[k] for k in self.plot_score['legend']])\n",
    "        self.vis.line(\n",
    "            X=np.stack([np.array(self.plot_score['X'])] * len(self.plot_score['legend']), 1),\n",
    "            Y=np.array(self.plot_score['Y']),\n",
    "            opts={\n",
    "                'title': self.name + ' Inception Score over time',\n",
    "                'legend': self.plot_score['legend'],\n",
    "                'xlabel': 'epoch',\n",
    "                'ylabel': 'score'},\n",
    "            win=self.display_id + 29\n",
    "        )\n",
    "\n",
    "    # statistics distribution: draw data histogram\n",
    "    def plot_current_distribution(self, distribution):\n",
    "        name = list(distribution.keys())\n",
    "        value = np.array(list(distribution.values())).swapaxes(1, 0)\n",
    "        self.vis.boxplot(\n",
    "            X=value,\n",
    "            opts=dict(legend=name),\n",
    "            win=self.display_id+30\n",
    "        )\n",
    "\n",
    "    # errors: same format as |errors| of plotCurrentErrors\n",
    "    def print_current_errors(self, epoch, i, errors, t):\n",
    "        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n",
    "        for k, v in errors.items():\n",
    "            message += '%s: %.3f ' % (k, v)\n",
    "\n",
    "        print(message)\n",
    "        with open(self.log_name, \"a\") as log_file:\n",
    "            log_file.write('%s\\n' % message)\n",
    "\n",
    "    # save image to the disk\n",
    "    def save_images(self, webpage, visuals, image_path):\n",
    "        image_dir = webpage.get_image_dir()\n",
    "        short_path = ntpath.basename(image_path[0])\n",
    "        name = os.path.splitext(short_path)[0]\n",
    "\n",
    "        webpage.add_header(name)\n",
    "        ims = []\n",
    "        txts = []\n",
    "        links = []\n",
    "\n",
    "        for label, image_numpy in visuals.items():\n",
    "            image_name = '%s_%s.png' % (name, label)\n",
    "            save_path = os.path.join(image_dir, image_name)\n",
    "            util.save_image(image_numpy, save_path)\n",
    "\n",
    "            ims.append(image_name)\n",
    "            txts.append(label)\n",
    "            links.append(image_name)\n",
    "        webpage.add_images(ims, txts, links, width=self.win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53e939d1-d12c-4420-97c0-d4cbad319491",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "/data/dataset/train is not a valid directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m opt \u001b[38;5;241m=\u001b[39m TrainOptions()\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# create a dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m*\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatchSize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining images = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_size)\n",
      "Cell \u001b[0;32mIn[12], line 136\u001b[0m, in \u001b[0;36mdataloader\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataloader\u001b[39m(opt):\n\u001b[0;32m--> 136\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mCreateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(datasets, batch_size\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mbatchSize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mno_shuffle, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(opt\u001b[38;5;241m.\u001b[39mnThreads), pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mCreateDataset.__init__\u001b[0;34m(self, opt, debug)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m=\u001b[39m opt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m debug\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_paths, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# provides random file for training and testing\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mmask_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(path_files)\u001b[0m\n\u001b[1;32m     16\u001b[0m     paths, size \u001b[38;5;241m=\u001b[39m make_dataset_txt(path_files)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     paths, size \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paths, size\n",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m, in \u001b[0;36mmake_dataset_dir\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m:param dir: directory paths that store the image\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m:return: image paths and sizes\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m img_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mdir\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid directory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mdir\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, _, fnames \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fnames):\n",
      "\u001b[0;31mAssertionError\u001b[0m: /data/dataset/train is not a valid directory"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Options--------------\n",
      "batchSize: 10\n",
      "checkpoints_dir: ./checkpoints\n",
      "continue_train: False\n",
      "detach_embedding: False\n",
      "display_freq: 100\n",
      "display_id: 1\n",
      "display_port: 8097\n",
      "display_single_pane_ncols: 0\n",
      "display_winsize: 256\n",
      "dynamic_sigma: False\n",
      "fineSize: [256, 256]\n",
      "gan_mode: lsgan\n",
      "gpu_ids: -1\n",
      "img_file: /data/dataset/train\n",
      "isTrain: True\n",
      "iter_count: 1\n",
      "lambda_gan: 1.0\n",
      "lambda_gen_l1: 20.0\n",
      "lambda_kl: 20.0\n",
      "lambda_match: 0.1\n",
      "lambda_rec_l1: 20.0\n",
      "loadSize: [266, 266]\n",
      "lr: 0.0001\n",
      "lr_policy: lambda\n",
      "mask_file: none\n",
      "mask_type: [1, 2, 3]\n",
      "model: tdanet\n",
      "nThreads: 8\n",
      "name: experiment_name\n",
      "niter: 5000000\n",
      "niter_decay: 0\n",
      "no_augment: False\n",
      "no_flip: False\n",
      "no_html: False\n",
      "no_maxpooling: False\n",
      "no_rotation: False\n",
      "no_shuffle: False\n",
      "output_scale: 4\n",
      "print_freq: 100\n",
      "prior_alpha: 0.8\n",
      "prior_beta: 8\n",
      "resize_or_crop: resize_and_crop\n",
      "save_iters_freq: 10000\n",
      "save_latest_freq: 1000\n",
      "text_config: config.bird.yml\n",
      "train_paths: two\n",
      "update_language: False\n",
      "valid_file: /data/dataset/valid\n",
      "which_iter: latest\n",
      "----------------End----------------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "/data/dataset/train is not a valid directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m opt \u001b[38;5;241m=\u001b[39m TrainOptions()\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# create a dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m*\u001b[39m opt\u001b[38;5;241m.\u001b[39mbatchSize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining images = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_size)\n",
      "Cell \u001b[0;32mIn[12], line 136\u001b[0m, in \u001b[0;36mdataloader\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataloader\u001b[39m(opt):\n\u001b[0;32m--> 136\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mCreateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(datasets, batch_size\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mbatchSize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mno_shuffle, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(opt\u001b[38;5;241m.\u001b[39mnThreads), pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mCreateDataset.__init__\u001b[0;34m(self, opt, debug)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m=\u001b[39m opt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m debug\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_paths, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# provides random file for training and testing\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mmask_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(path_files)\u001b[0m\n\u001b[1;32m     16\u001b[0m     paths, size \u001b[38;5;241m=\u001b[39m make_dataset_txt(path_files)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     paths, size \u001b[38;5;241m=\u001b[39m \u001b[43mmake_dataset_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paths, size\n",
      "Cell \u001b[0;32mIn[13], line 47\u001b[0m, in \u001b[0;36mmake_dataset_dir\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m:param dir: directory paths that store the image\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m:return: image paths and sizes\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m img_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mdir\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid directory\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mdir\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, _, fnames \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fnames):\n",
      "\u001b[0;31mAssertionError\u001b[0m: /data/dataset/train is not a valid directory"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "import time\n",
    "s = '--name tda_bird  --gpu_ids 0 --model tdanet --mask_type 0 1 2 3 --img_file ./datasets/CUB_200_2011/train.flist --mask_file ./datasets/CUB_200_2011/train_mask.flist --text_config config.bird.yml'\n",
    "opt = TrainOptions().parse()\n",
    "# create a dataset\n",
    "dataset = dataloader(opt)\n",
    "dataset_size = len(dataset) * opt.batchSize\n",
    "print('training images = %d' % dataset_size)\n",
    "# create a model\n",
    "model = create_model(opt)\n",
    "# create a visualizer\n",
    "visualizer = Visualizer(opt)\n",
    "# training flag\n",
    "keep_training = True\n",
    "max_iteration = opt.niter+opt.niter_decay\n",
    "epoch = 0\n",
    "total_iteration = opt.iter_count\n",
    "\n",
    "# training process\n",
    "while(keep_training):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch+=1\n",
    "    print('\\n Training epoch: %d' % epoch)\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        dataset.epoch = epoch - 1\n",
    "        iter_start_time = time.time()\n",
    "        total_iteration += 1\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        # display images on visdom and save images\n",
    "        if total_iteration % opt.display_freq == 0:\n",
    "            visualizer.display_current_results(model.get_current_visuals(), model.get_current_text(), epoch)\n",
    "            visualizer.plot_current_distribution(model.get_current_dis())\n",
    "\n",
    "        # print training loss and save logging information to the disk\n",
    "        if total_iteration % opt.print_freq == 0:\n",
    "            losses = model.get_current_errors()\n",
    "            t = (time.time() - iter_start_time) / opt.batchSize\n",
    "            visualizer.print_current_errors(epoch, total_iteration, losses, t)\n",
    "            if opt.display_id > 0:\n",
    "                visualizer.plot_current_errors(total_iteration, losses)\n",
    "\n",
    "        # save the latest model every <save_latest_freq> iterations to the disk\n",
    "        if total_iteration % opt.save_latest_freq == 0:\n",
    "            print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_iteration))\n",
    "            model.save_networks('latest')\n",
    "\n",
    "        # save the model every <save_iter_freq> iterations to the disk\n",
    "        if total_iteration % opt.save_iters_freq == 0:\n",
    "            print('saving the model of iterations %d' % total_iteration)\n",
    "            model.save_networks(total_iteration)\n",
    "\n",
    "        if total_iteration > max_iteration:\n",
    "            keep_training = False\n",
    "            break\n",
    "\n",
    "    model.update_learning_rate()\n",
    "\n",
    "    print('\\nEnd training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3a7c300-7a59-49d7-9a0d-133074278da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mo\n"
     ]
    }
   ],
   "source": [
    "print('mo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
